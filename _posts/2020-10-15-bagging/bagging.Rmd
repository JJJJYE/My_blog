---
title: "Bagging"
description: |
   R code for Random Forest of Bagging
author:
  - name: Yeongeun Jeon
  - name: Jeongwook Lee
  - name: Jung In Seo
date: 10-15-2020
preview: preview.PNG
categories: Machine Learning
output: 
  distill::distill_article:
        toc: TRUE
---


```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE,
	                    message = FALSE,
	                    warning = FALSE)
```

> Bagging은 Bootstrap을 이용하여 독립적으로 모형을 생성하는 알고리즘이며, 가장 대표적인 기법은 Random Forest이다. 예제 데이터를 이용해 Random Forest을 수행해보았다.  
예제 데이터는 "Universal Bank_Main"로 유니버셜 은행의 고객들에 대한 데이터(출처 : Data Mining for Business Intelligence, Shmueli et al. 2010)이다. 데이터는 총 2500개이며, 변수의 갯수는 13개이다. 여기서 **Target**은 `Person.Loan`이다.


<center><img src="./image/그림1.png" width="600" height="600"></center>

<br />


<center><img src="./image/표.png" width="400" height="400"></center>

----------

# **1. 데이터 불러오기**

```{r}
pacman::p_load("data.table", "dplyr")     

UB   <- fread(paste(getwd(),"Universal Bank_Main.csv", sep="/")) %>%   # 데이터 불러오기
  data.frame() %>%                                                     # Data frame 변환
  mutate(Personal.Loan = ifelse(Personal.Loan==1, "yes","no")) %>%     # Character for classification
  select(-1)                                                           # ID변수 제거



cols <- c("Family", "Education", "Personal.Loan", "Securities.Account", 
          "CD.Account", "Online", "CreditCard")

UB   <- UB %>% 
  mutate_at(cols, as.factor)                                          # 범주형 변수 변환

glimpse(UB)                                                           # 데이터 구조            

```


-----------

# **2. 데이터 분할**

```{r}
pacman::p_load("caret")
# Partition (Traning Data : Test Data = 7:3)
y      <- UB$Personal.Loan                       # Target

set.seed(200)
ind    <- createDataPartition(y, p=0.7, list=F)  # Training Data를 70% 추출

UB.trd <- UB[ind,]                               # Traning Data

UB.ted <- UB[-ind,]                              # Test Data

detach(package:caret)
```


----------

# **3. Random Forest**

>Bagging에서 가장 많이 쓰이는 기법은 Random Forest이다. Random Forest를 수행할 수 있는 Package는 `"randomForest"`, `"party"`가 있으며, 예제 데이터에는 `"randomForest"`를 사용하였다. 자세한 내용은 [여기를 참고한다.](https://www.rdocumentation.org/packages/randomForest/versions/4.6-14/topics/randomForest)

```{r, eval=FALSE}
randomForest(formula, data, ntree, importance, mtry, ...)
```


- `formula` : Target과 예측 변수에 대한 공식으로써 일반적으로 `Target ~ 예측변수` 사용
- `data` : `formula`의 변수들이 있는 데이터 프레임
-  `ntree` : 생성할 나무 갯수
- `importance` : `TRUE` 일 때 예측변수에 대한 중요도를 평가
- `mtry` : 분할할 때마다 랜덤적으로 추출할 예측변수 갯수


----------

## **3-1. 모형 적합**

```{r}
pacman::p_load("randomForest")

set.seed(100)
UB.rf <- randomForest(Personal.Loan~., data=UB.trd,
                      ntree=100, importance=T, mtry=sqrt(12))  # randomForest(formula, datda, ntree,mtry=sqrt(p))
                                                              
UB.rf
```


### **3-1-1. 변수 중요도**


```{r}
# 변수 중요도
varImpPlot(UB.rf)
```


### **3-1-2. OBB Error**

```{r}
head(UB.rf$err.rate)
```

```{r}
# Plot for Error
pacman::p_load("ggplot2")
oob.error.data <- data.frame(Trees=rep(1:nrow(UB.rf$err.rate),times=3), 
                             Type=rep(c("OOB","No","Yes"), 
                                      each=nrow(UB.rf$err.rate)),
                             Error=c(UB.rf$err.rate[,"OOB"],
                                     UB.rf$err.rate[,"no"],
                                     UB.rf$err.rate[,"yes"]))



ggplot(data=oob.error.data, aes(x=Trees, y=Error)) + 
  geom_line(aes(color=Type)) + theme_bw()

detach(package:ggplot2)

```

----------

## **3-2. 모형 평가**

```{r}
# 적합된 모형에 대하여 Test Data 예측
UB.pred.rf <- predict(UB.rf, newdata=UB.ted) 			# predict(Random Forest모형, Test Data)
```


### **ConfusionMatrix**

```{r}
pacman::p_load("caret")

confusionMatrix(UB.pred.rf, UB.ted$Personal.Loan, positive = "yes") # confusionMatrix(예측 클래스, 실제 클래스, positive = "관심 클래스")

```

<br />

### **ROC 곡선**


#### **1) Package "pROC"**
```{r}
pacman::p_load("pROC")                          

ac     <- UB.ted$Personal.Loan                              # 실제 클래스

pp     <- predict(UB.rf, newdata=UB.ted, type="prob")[,2]   # "yes"에 대한 예측 확률 출력


rf.roc <- roc(ac, pp, plot=T, col="red")                    # roc(실제 클래스, 예측 확률)

auc <- round(auc(rf.roc), 3)                                # AUC 
legend("bottomright",legend=auc, bty="n")

detach(package:pROC)
```

<br />

#### **2) Package "Epi"**

```{r}
# install.packages("Epi")
pacman::p_load("Epi")                        
# install_version("etm", version = "1.1", repos = "http://cran.us.r-project.org")

ROC(pp,ac, plot="ROC")       # ROC(예측 확률 , 실제 클래스)                                

detach(package:Epi)

```

<br />

#### **3) Package "ROCR"**

```{r}
pacman::p_load("ROCR")                      

rf.pred <- prediction(pp, ac)                       # prediction(예측 확률, 실제 클래스)
  
rf.perf <- performance(rf.pred, "tpr", "fpr")       # performance(, "민감도", "1-특이도")                      
plot(rf.perf, col="red")                            # ROC Curve
abline(0,1, col="black")


perf.auc <- performance(rf.pred, "auc")             # AUC        

auc <- attributes(perf.auc)$y.values                  
legend("bottomright",legend=auc,bty="n") 

```

<br />


### **향상 차트**


#### **1) Package "ROCR"**

```{r}
rf.lift <- performance(rf.pred,"lift", "rpp")       # Lift chart
plot(rf.lift, colorize=T, lwd=2)			

detach(package:ROCR)

```

<br />

#### **2) Package "lift"**

```{r}
pacman::p_load("lift")

ac.numeric <- ifelse(UB.ted$Personal.Loan=="yes",1,0)     # 실제 클래스를 수치형으로 변환

plotLift(pp, ac.numeric, cumulative = T, n.buckets =24)   # plotLift(예측 확률, 실제 클래스)
TopDecileLift(pp, ac.numeric)                             # Top 10% 향상도 출력

detach(package:lift)
```


