---
title: "Time Series Analysis Method Summary(2)"
description: |
  Time Series Analysis Method Summary
author:
  - name: Yeongeun Jeon
  - name: Jung In Seo
date: 10-02-2021
preview: preview.PNG
categories: Time Series
output: 
  distill::distill_article:
        toc: TRUE
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

- Data 출처 : [Data Mining for Business Analytics](https://www.dataminingbook.com/book/r-edition)에서 사용한 미국 철도 회사 “Amtrak”에서 수집한 1991년 1월~2004년 3월까지 매달 환승 고객 수

-----------------  

# **Data 불러오기**

```{r}
pacman::p_load( "dplyr", "xts", 
                "forecast", "dlm", "bsts",
                "caret", "caretEnsemble",
                "ggplot2")

library(doParallel)
library(parallel)

# cl <- makePSOCKcluster(detectCores())
# clusterEvalQ(cl, library(foreach))
# registerDoParallel(cores=cl)

registerDoParallel(cores=detectCores())


# Data 불러오기 ---------------------------------------------------------------

# In Mac
# guess_encoding("Amtrak.csv")
# Amtrak.data <- read.csv("Amtrak.csv", fileEncoding="EUC-KR")

Amtrak.data <- read.csv("C:/Users/User/Desktop/Amtrak.csv")
Amtrak.data$Month <- as.Date(Amtrak.data$Month, format = "%d/%m/%Y")               # For ggplot
ridership.ts <- ts(Amtrak.data$Ridership, start=c(1991,1), end=c(2004,3), freq=12)

```

----------------- 

# **Data 분할**

```{r}
train.ts     <- window(ridership.ts,start=c(1991,1), end=c(2001,3))   # Training Data
test.ts      <- window(ridership.ts,start=c(2001,4))                  # Test Data
n.test       <- length(test.ts)
```

----------------- 

# **예측 변수 생성**

```{r}
# 1. Fourier Term
FT      <- fourier(train.ts, K=2)                    # K : sine, cosine 쌍의 개수/시계열 데이터의 계절 주기가 2개 이상일 때, K는 계절 주기 수만큼 필요
FT.Test <- fourier(train.ts, K=2, h=n.test)

# 2. Month
xts(ridership.ts, order = as.Date(ridership.ts))
Month  <- as.Date(ridership.ts) %>%                  # Date 추출
  lubridate::month()                                 # Month 추출

## 퓨리에 항과 합치기
Train.X <- cbind("Month"= Month[1:length(train.ts)], FT) 
Test.X  <- cbind("Month"= Month[-(1:length(train.ts))], FT.Test) 

```


----------------- 

# **전통적 시계열 모형**

- 전통적인 방법으로 사용될 모형은 다음과 같다. 
    - Regression with ARIMA error
    - Dynamic Linear Model
    - Bootstrap and Bagging
    - Dynamic Harmonic Model
    - STLM
    - Baysian Structural Time Series Model
    - TBATS

----------------- 

## **1. Regression with ARIMA error**

- `forecast` package에 있는 `tslm()` 함수를 이용하여 추세와 계절성이 동시에 존재하는 회귀모형을 적합시켜보았다.
- 추세는 U자형으로 2차함수 형태의 추세를 고려하였다.

----------------- 

### **모형 적합**

```{r}
# Fit Trend + Seasonality Model
train.lm.trend.season <- tslm(train.ts ~ trend + I(trend^2) + season)  
summary(train.lm.trend.season)

# Forecast
train.lm.trend.season.pred <- forecast(train.lm.trend.season, h=n.test, level=0) 
train.lm.trend.season.pred

checkresiduals(train.lm.trend.season) 
```

- 모형 적합 후 잔차의 자기상관 그래프를 보면 잔차들이 독립이 아닌 것을 볼 수 있다.
   - 잔차는 white noise로써 독립을 만족해야한다.
- 정상성 시계열로 데이터를 변형하는 것보다 잔차에 회귀모형을 적합시키는 것은 데이터 변형을 필요로 하지 않는다.
- `잔차에 회귀모형을 적합`시킴으로써 예측값을 향상시킬 수 있다.
   - 짧은 기간의 예측에 유용하다.

```{r}
# Fit ARIMA Model to training residuals
train.res.arima <- auto.arima(train.lm.trend.season$residuals)
summary(train.res.arima)

train.res.arima.pred <- forecast(train.res.arima, h=n.test, level=0)
train.res.arima.pred$mean
```

```{r}
checkresiduals(train.res.arima$residuals, lag.max = 12)   # Uncorrelated.
```

----------------- 

### **예측**

```{r}
# Final Prediction
res.arima <- as.data.frame(train.res.arima.pred)[,1]
fore      <- as.data.frame(train.lm.trend.season.pred)[,1]
Improved_forecast <- apply(cbind(res.arima,fore), 1,sum)            # 잔차의 예측값과 Training Data의 적합값을 더함
Improved_forecast
```

----------------- 

### **Accuracy**

```{r}
Improved_forecast <- ts(Improved_forecast, start=c(2001,4), end=c(2004,3), freq=12)  # Convert as time-series

# Accuracy
acc.Regression <- accuracy(Improved_forecast, test.ts)
acc.Regression
```

----------------- 

### **예측 그래프**

```{r}
# Prediction Plot
ggplot(tail(Amtrak.data, n.test), aes(x = Month)) +
  geom_line(aes(y = Ridership, colour = "Observation", linetype = "Observation")) + 
  geom_line(aes(y = Improved_forecast, colour = "Prediction", linetype = "Prediction")) +
  scale_color_manual(name = "", values = c("Observation" = "black", "Prediction" = "blue")) +
  scale_linetype_manual("", values = c("Observation"= 1, "Prediction" = 2)) +
  theme_bw()
```

----------------- 

## **2. Dynamic Linear Model**

- `dlm` package 안에 있는 여러 함수들을 이용하여 추정과 예측을 실시할 수 있다.
- DLM은 상태공간모형의 종류로, 선형성과 정규성을 가정하고 있다. 
- DLM은 시간에 따라 변화하는 회귀계수를 가지는 선형회귀모형의 일반화 형태이다.
 
----------------- 

### **모형 적합**

```{r}
# Fit Dynamic linear model (DLM) 
DLM.model <- function(p){
  
  mod <- dlmModPoly(2) +  # local trend linear 
    dlmModSeas(12)
  
  V(mod)            <- exp(p[1])
  diag(W(mod))[1:3] <- exp(p[2:4])
  
  return(mod)  
}

mle1 <- dlmMLE(train.ts, parm = c(0.1,0.1,1,1), build = DLM.model )        # Estimation parameter through MLE. Parameter= Variance of error
ifelse(mle1$convergence==0, print("converge"), print("did not converge") ) # Check Convergence

DLM.model.fit <- DLM.model(mle1$par)  # Fitting the DLM 
V(DLM.model.fit)
W(DLM.model.fit)
```

----------------

### **상태 추정**

- 관측되지 않은 상태 (예: 추세, 계절성)를 추정하기 위해 Kalman Filtering 방법을 사용하였다.

```{r}
# Estimation for Kalman filtering
filtering <- dlmFilter(train.ts, DLM.model.fit)

# Plot estimation for filtering model
plot(dropFirst(filtering$f), col = "blue", lwd = 2, lty = 2, ylab = "Ridership")
lines(train.ts ,lty = 1, lwd = 2, col = "black") 
legend("bottomleft", legend = c("Data", "Fitted Data"), col = c("black", "blue"), lty = 1:2, lwd = 2)

# Plot for estimated states
par(mfrow=c(1,3))
plot(dropFirst(filtering$m[,1]), ylab = "Level")
plot(dropFirst(filtering$m[,2]), ylab = "Slope")
plot(dropFirst(filtering$m[,3]), ylab = "Seasonality")

```

----------------

### **정규성 확인**

- DLM은 정규성을 가정하기 때문에 오차에 대해 정규성 확인이 필요하다.

```{r}
# Residual(Check independence and normality)
par(mfrow=c(1,1))
plot(residuals(filtering, sd = FALSE), ylab="Residual")
abline(h=0)
tsdiag(filtering, main = "Diagnostics for Regression Model")

# Check normality for error
qqnorm(residuals(filtering, sd = FALSE))
qqline(residuals(filtering, sd = FALSE))
```


----------------- 

### **예측**

```{r}
# Forecast
forecast_DLM <- dlmForecast(filtering, nAhead = n.test)  # Forecast(filtering model)
forecast_DLM$f
```

----------------- 

### **Accuracy**

```{r}
# Accuracy
acc.DLM <- accuracy(forecast_DLM$f, test.ts)
acc.DLM
```

----------------- 

### **예측 그래프**

```{r}
# Prediction Plot
ggplot(tail(Amtrak.data, n.test), aes(x = Month)) +
  geom_line(aes(y = Ridership, colour = "Observation", linetype = "Observation")) + 
  geom_line(aes(y = forecast_DLM$f, colour = "Prediction", linetype = "Prediction")) +
  scale_color_manual(name = "", values = c("Observation" = "black", "Prediction" = "blue")) +
  scale_linetype_manual("", values = c("Observation"= 1, "Prediction" = 2)) +
  theme_bw()
```

----------------- 

## **3. Bootstrap and Bagging**

- 붓스트랩과 Bagging 방법을 이용하여 시계열 데이터를 추정하고 예측한다.
- 붓스트랩과 Bagging 방법의 가장 큰 핵심은 예측구간을 만들 수 있다는 것과 bagging을 이용하여 점 예측을 향상시킬 수 있다는 것이다.

------------------

### **모형 적합**

```{r}
set.seed(100)
bagged <- train.ts %>%
  baggedModel(bld.mbb.bootstrap(train.ts, 100),fn = auto.arima)   # Generate 100set Bootstrap Samples and Fitting auto.arima for Each Bootstrap Sample Set 

```

----------------- 

### **예측**

```{r}
# Forecast
forecast_bagged <- bagged %>% 
  forecast(h = n.test)  # Auto.arima is more accurate than ets. / Forecaste Each Fitted ARIMA Model by bootstrap sample set. Then, Calculate Mean
forecast_bagged$mean
```

----------------- 

### **Accuracy**

```{r}
# Accuracy
acc.Bagging <- accuracy(forecast_bagged$mean, test.ts)
acc.Bagging
```

----------------- 

### **예측 그래프**

```{r}
# Prediction Plot
ggplot(tail(Amtrak.data, n.test), aes(x = Month)) +
  geom_line(aes(y = Ridership, colour = "Observation", linetype = "Observation")) + 
  geom_line(aes(y = forecast_bagged$mean, colour = "Prediction", linetype = "Prediction")) +
  scale_color_manual(name = "", values = c("Observation" = "black", "Prediction" = "blue")) +
  scale_linetype_manual("", values = c("Observation"= 1, "Prediction" = 2)) +
  theme_bw()
```

----------------- 

## **4. Dynamic Harmonic Model**

- `계절 주기가 길 때`, 계절성 ARIMA보다 선호되는 모델이 `Dynamic Harmonic Regression (DHR)`이다.
- DHR은 sine과 cosine 함수의 선형 조합으로써 `계절성을 설명하는 퓨리에 항(Fourier Terms)`을 가진 회귀 모형이다.

------------------

### **모형 적합**

```{r}
DHR.fit <- auto.arima(train.ts, 
                      xreg = as.matrix(Train.X),      # Include Fourier Terms 
                      seasonal = FALSE)
DHR.fit
checkresiduals(DHR.fit)
```

----------------- 

### **예측**

```{r}
# Forecast
DHR.forecast <- forecast(DHR.fit, xreg = as.matrix(Test.X))
DHR.forecast$mean
```

----------------- 

### **Accuracy**

```{r}
# Accuracy
acc.DHR <- accuracy(DHR.forecast$mean, test.ts)
acc.DHR
```

----------------- 

### **예측 그래프**

```{r}
# Prediction Plot
ggplot(tail(Amtrak.data, n.test), aes(x = Month)) +
  geom_line(aes(y = Ridership, colour = "Observation", linetype = "Observation")) + 
  geom_line(aes(y = DHR.forecast$mean, colour = "Prediction", linetype = "Prediction")) +
  scale_color_manual(name = "", values = c("Observation" = "black", "Prediction" = "blue")) +
  scale_linetype_manual("", values = c("Observation"= 1, "Prediction" = 2)) +
  theme_bw()
```

----------------- 

## **5. STLM**

- `STL (Seasonal and Trend decomposition using Loess)` 을 이용하여 시계열 데이터를 계절 성분(Seasonal Component)과 추세 + 오차 성분(Seasonally Adjusted Component)로 분해하고 각 성분을 따로 예측 후 더하여 최종 예측이 이루어진다.
- 계절 성분은 Seasonal naive method로, 추세 + 오차 성분은 시계열 모형으로 예측한다.

------------------

### **모형 적합**

```{r}
# Ref. https://github.com/robjhyndman/forecast/blob/master/R/mstl.R

STLM.fit <- train.ts %>%
  stlm(method = "arima", xreg = Train.X)            # 시계열을 분해하고 추세 + 오차 성분에 DHR 모형 적합 => 예측 변수에 Fourier Terms
STLM.fit$model
```

----------------- 

### **예측**

```{r}
# Forecast
STLM.forecast <- forecast(STLM.fit, h = n.test,     # Trend + Error부분을 예측하고 Seasonal naive method(같은 시즌의 마지막 관측값=예측)를 이용하여 Seasonal 예측하여 더함
                          newxreg = Test.X)    
STLM.forecast$mean
```

----------------- 

### **Accuracy**

```{r}
# Accuracy
acc.STLM <- accuracy(STLM.forecast$mean, test.ts)
acc.STLM
```

----------------- 

### **예측 그래프**

```{r}
# Prediction Plot
ggplot(tail(Amtrak.data, n.test), aes(x = Month)) +
  geom_line(aes(y = Ridership, colour = "Observation", linetype = "Observation")) + 
  geom_line(aes(y = STLM.forecast$mean, colour = "Prediction", linetype = "Prediction")) +
  scale_color_manual(name = "", values = c("Observation" = "black", "Prediction" = "blue")) +
  scale_linetype_manual("", values = c("Observation"= 1, "Prediction" = 2)) +
  theme_bw()
```

----------------- 

## **6. Baysian Structural Time Series Model**

- Bayesian Structural Time Series (BSTS)는 `Structural Time Seires (STS) 모형에 Bayesian 방법을 적용`하는 방법이다.
- BSTS는 R package `bsts`를 통해 다룰 수 있다.

------------------

### **모형 적합**

```{r}
Train.Data <- data.frame("y"= train.ts, Train.X[,1])     # Excluding Fourier Terms

ss <- list()
# Local Linear Trend
ss <- bsts::AddLocalLinearTrend(ss, train.ts)
# Seasonal
ss <- bsts::AddSeasonal(ss, train.ts, nseasons = 12, season.duration = 1) # cycle = season.duration(how many time points each season) * nseasons

BSTS.fit <- bsts(y ~., state.specification = ss, data = Train.Data, niter = 1000, seed=100)  # niter : MCMC 반복
summary(BSTS.fit)

```

----------------- 

### **예측**

```{r}
# Forecast
burn <- SuggestBurn(0.1, BSTS.fit)
BSTS.forecast <- predict(BSTS.fit, horizon = n.test,             # horizon : the number of prediction
                         burn = burn, newdata = Test.X[,1],      # newdata : 예측변수를 포함하는 변수
                         quantiles = c(0.025, 0.975))  
BSTS.forecast$mean
```

----------------- 

### **Accuracy**

```{r}
# Accuracy
acc.BSTS <- accuracy(BSTS.forecast$mean, test.ts)
acc.BSTS
```

----------------- 

### **예측 그래프**

```{r}
# Prediction Plot
ggplot(tail(Amtrak.data, n.test), aes(x = Month)) +
  geom_line(aes(y = Ridership, colour = "Observation", linetype = "Observation")) + 
  geom_line(aes(y = BSTS.forecast$mean, colour = "Prediction", linetype = "Prediction")) +
  scale_color_manual(name = "", values = c("Observation" = "black", "Prediction" = "blue")) +
  scale_linetype_manual("", values = c("Observation"= 1, "Prediction" = 2)) +
  theme_bw()
```

-----------------

## **7. TBATS**

- TBATS 모형은 `복잡한 계절성`을 가진 시계열 데이터를 분석하는 데 유용하다.
- TBATS 모형은 `광범위한 계절 패턴 변동`과 관련된 문제를 극복하고 `상관성이 있는 오차`를 처리하기 위해 `지수 평활(Exponential Smoothing)`을 사용한 수정된 상태공간 모형으로 [De Livera et al. (2011)](https://robjhyndman.com/papers/ComplexSeasonality.pdf)이 제안하였다

------------------

### **모형 적합**

```{r}
cl <- parallel::makeCluster(detectCores(), setup_timeout = 0.5)
TBATS.fit <- train.ts %>%
  tbats(use.box.cox = FALSE,
        use.trend = TRUE,
        use.damped.trend = TRUE,
        use.parallel = TRUE,
        num.cores = cl)

summary(TBATS.fit)

```

----------------- 

### **예측**

```{r}
# Forecast
TBATS.forecast <- forecast(TBATS.fit, h=n.test)
TBATS.forecast$mean
```

----------------- 

### **Accuracy**

```{r}
# Accuracy
acc.TBATS <- accuracy(TBATS.forecast$mean, test.ts)
acc.TBATS
```

----------------- 

### **예측 그래프**

```{r}
# Prediction Plot
ggplot(tail(Amtrak.data, n.test), aes(x = Month)) +
  geom_line(aes(y = Ridership, colour = "Observation", linetype = "Observation")) + 
  geom_line(aes(y = TBATS.forecast$mean, colour = "Prediction", linetype = "Prediction")) +
  scale_color_manual(name = "", values = c("Observation" = "black", "Prediction" = "blue")) +
  scale_linetype_manual("", values = c("Observation"= 1, "Prediction" = 2)) +
  theme_bw()
```

-----------------

## **예측 비교1**

### **Line Plot**

```{r}
# 1. Line Plot
Pred.Data <- data.frame("Date" = tail(Amtrak.data$Month, n.test),
                        "Observation" = test.ts,
                        "Regression" = Improved_forecast,
                        "DLM" = c(forecast_DLM$f),
                        "Bootstrap" = forecast_bagged$mean,
                        "DHR" = DHR.forecast$mean,
                        "STLM" = STLM.forecast$mean,
                        "BSTS" = BSTS.forecast$mean,
                        "TBATS" = TBATS.forecast$mean)


Pred.Models <- reshape2::melt(Pred.Data, id.vars="Date",
                              variable.name="Type",
                              value.name='y')

models <- unique(Pred.Models$Type)

lcols <- c("#000000", "#66CC99", "#ae5d8b", "#c1c084", "#d38b72", "#dc143c", "#00498c", "#9999CC")

ggplot(Pred.Models, aes(x=Date, y=y, group=Type)) +
  geom_line(aes(color=Type), size=1, linetype="solid") +
  labs(x="Date", y="Ridership") +                # y="log(Charging demand)"
  scale_color_manual(values = lcols, name = 'Models', labels = models) +
  #scale_x_datetime(date_breaks= "24 hour", date_labels = paste("%Y- %m-%d %H", "h", sep="")) +  # %Y- %m-%d %H:%M:%S"
  theme_bw() +
  theme(axis.text.y=element_text(size=16),
        axis.title=element_text(size=20),
        axis.text.x=element_text(angle=30, hjust=1, size=12),
        legend.title = element_text(size=18),
        legend.text = element_text(size=16)) 

```

-----------------

### **Scatter Plot**


```{r}
# 2. Scatter Plot
Pred.Models2 <- reshape2::melt(Pred.Data[,-1], id.vars="Observation",
                              variable.name="Type",
                              value.name='Pred')

ggplot(Pred.Models2, aes(x=Observation, y=Pred)) + 
  geom_point(color="#d38b72") +
  geom_abline(intercept = 0, slope = 1, size=1, colour="#9999CC", linetype="dashed") +
  facet_wrap( ~ Type, ncol=2) +
  labs(x="Observation", y="Prediction", color = "") +
  theme_bw() +
  theme(axis.title=element_text(size=20),
        axis.text=element_text(size=15),
        strip.text.x = element_text(size=18, face="bold")) 
```


-----------------

# **머신 러닝 기법**

- 머신 러닝 기법에서 예제 데이터에 적용되는 기법은 다음과 같다.
    - Neural Network Model
    - Random Forest
    - XGBoost (eXtreme Gradient Boosting)
    
-----------------

## **분해**

- 머신 러닝 기법 중 트리 기반 모델은 추세(Trend)를 포착하지 못하기 때문에, 추세를 제거한 Detrend Dataset에 머신 러닝 기법을 적용하고 추세는 따로 모델링을 해야한다.


```{r}
# Ref. https://petolau.github.io/Regression-trees-for-forecasting-time-series-in-R/

decomp.ts <- stl(train.ts, , s.window = "periodic", robust = TRUE)$time.series 
# decomp.ts <- mstl(Power.msts, s.window = "periodic", robust = TRUE) # 다중 계절성인 경우

# Target without Trend
Target <- decomp.ts %>% 
  data.frame %>%
  rowwise() %>%                                        # 행별로 작업
  dplyr::mutate(y=sum( seasonal, remainder )) %>%      # Target = Season + Remainder => Detrend
  dplyr::select(y)


Train.Data <- cbind(Target, Train.X)
```

-----------------

## **추세**

- 추세는 ARIMA 모형을 이용하여 예측하였다.


```{r}
trend.part  <- data.frame(decomp.ts)$trend %>% # Only Trend of Training Data
  ts()

# Fitting ARIMA for Trend 
trend.fit.arima   <- auto.arima(trend.part)

# Forecast 
trend.arima.pred  <- forecast(trend.fit.arima, n.test)
trend.arima.pred$mean 

```

-----------------


## **1. Neural Network Model**

- 시계열의 `시차값`이 `신경망의 입력값`으로써 사용될 수 있으며 이것을 신경망 자기회귀 또는 NNAR 모형이라고 부른다.
- NNAR($p,k$)는 출력값 $y_{t}$를 예측하기 위해 마지막 $p$개의 관측값 ($y_{t-1}$, $y_{t-2}$, $\ldots$, $y_{t-p}$)을 입력값으로 사용하고 숨겨진 계층에 $k$개의 뉴런이 있는 신경망이다. 
- 계절성이 있는 경우 모형은 NNAR$(p,P,k)_{m}$로써 $P$는 입력값으로써 사용된 계절성 시차의 수이며 $m$은 주기를 의미한다. 
    - NNAR$(p,P,k)_{m}$은 $y_{t-1}$, $\ldots$, $y_{t-p}$, $y_{t-m}$, $y_{t-2m}$, $\ldots$, $y_{t-mP}$를 입력값으로써 사용하고, 숨겨진 계층에 $k$개의 뉴런이 있다.
    
------------------

### **모형 적합**

```{r}
set.seed(100)
neural_model <- nnetar(ts(Target, start=c(1991,1), end=c(2001,3), freq = 12),    # Or train.ts
                       xreg = Train.X,
                       repeats = 200, 
                       lambda = "auto") 

neural_model
```

----------------- 

### **예측**

```{r}
# Forecast for Detrended
forecast_neu <- forecast(neural_model, PI = TRUE,  # PI : Confidencd Interval
                         h = n.test,
                         xreg = Test.X,
                         npaths = 100, level = 95) # Npaths : How many simulation

# Final Prediction (Detrended + Trend)
NNAR.forecast <- c(forecast_neu$mean) + c(trend.arima.pred$mean) 
```

----------------- 

### **Accuracy**

```{r}
# Accuracy
acc.NNAR <- accuracy(NNAR.forecast, test.ts)
acc.NNAR
```

----------------- 

### **예측 그래프**

```{r}
# Prediction Plot
ggplot(tail(Amtrak.data, n.test), aes(x = Month)) +
  geom_line(aes(y = Ridership, colour = "Observation", linetype = "Observation")) + 
  geom_line(aes(y = NNAR.forecast, colour = "Prediction", linetype = "Prediction")) +
  scale_color_manual(name = "", values = c("Observation" = "black", "Prediction" = "blue")) +
  scale_linetype_manual("", values = c("Observation"= 1, "Prediction" = 2)) +
  theme_bw()
```

-----------------

## **2. Random Forest**

- Bagging을 이용한 트리 기반 모형이다.
- 나무를 분할할 때 랜덤적으로 후보 예측 변수를 선택함으로써, 생성된 나무들의 연관성은 감소된다.
- Hyperparameter로 `mtry` (랜덤적으로 선택되는 후보 예측 변수 갯수)를 가진다.

------------------

### **모형 적합**

```{r}
set.seed(100)
fitControl <- trainControl(method = "adaptive_cv",   # cv, repeatedcv
                           number = 5,
                           repeats = 5,
                           adaptive = list(min = 5,
                                           alpha = 0.05,
                                           method = "BT",
                                           complete = TRUE),
                           search = "random",
                           allowParallel = TRUE) 

RF <- function(train, tuneLength, ntree = 500, nodesize = 5){
  
  set.seed(100)                                        # seed 고정 For Cross Validation
  caret.rf <- caret::train(y~., data = train, 
                           method = "parRF",           # Tune Parameter : mtry
                           trControl = fitControl,
                           tuneLength = tuneLength,   
                           ntree = ntree,             
                           nodesize = nodesize,        # nodesize : Terminal Node의 최소 크기
                           importance = TRUE)   
  
  return(caret.rf)
  
}

RF.Caret <- RF(Train.Data, 
               tuneLength = 2,     # tuneLength (탐색할 후보 모수 갯수)
               ntree = 100)        # ntree : 생성할 Tree 수

RF.Caret
RF.Caret$finalModel
RF.Caret$finalModel$tuneValue
```

----------------- 

### **예측**

```{r}
# Final Prediction (Detrended + Trend)
Pred.RF <- predict(RF.Caret, Test.X) + trend.arima.pred$mean  
```

----------------- 

### **Accuracy**

```{r}
# Accuracy
acc_RF  <- accuracy(c(Pred.RF), test.ts)
acc_RF
```

----------------- 

### **예측 그래프**

```{r}
# Prediction Plot
ggplot(tail(Amtrak.data, n.test), aes(x = Month)) +
  geom_line(aes(y = Ridership, colour = "Observation", linetype = "Observation")) + 
  geom_line(aes(y = Pred.RF, colour = "Prediction", linetype = "Prediction")) +
  scale_color_manual(name = "", values = c("Observation" = "black", "Prediction" = "blue")) +
  scale_linetype_manual("", values = c("Observation"= 1, "Prediction" = 2)) +
  theme_bw()
```

-----------------

## **3. XGBoost**

- Boosting을 이용한 트리 기반 모형이다.
- 손실함수와 경사하강법을 이용하는 Gradient Boosting의 단점을 해결하기 위해 제안되었다.
- 가장 큰 특징으로는, 병렬 처리로 인해 빠르고 조기 종료가 가능하다는 것이다.
- XGBoost에서 Hyperparameter는 다음과 같다.
   - `nrounds` : 반복 수
   - `max_depth` : Tree의 최대 깊이
   - `eta` : Learning Late
   - `gamma` : 분할하기 위해 필요한 최소 손실 감소, 클수록 분할이 쉽게 일어나지 않음
   - `colsample_bytree` : Tree 생성 때 사용할 예측변수 비율 
   - `min_child_weight` : 한 leaf 노드에 요구되는 관측치에 대한 가중치의 최소 합
   - `subsample` : 모델 구축시 사용할 Data비율로 1이면 전체 Data 사용
   
   
------------------

### **모형 적합**

```{r}
set.seed(100)
fitControl <- trainControl(method = "adaptive_cv",   # cv, repeatedcv
                           number = 5,
                           repeats = 5,
                           adaptive = list(min = 5,
                                           alpha = 0.05,
                                           method = "BT",
                                           complete = TRUE),
                           search = "random",
                           allowParallel = TRUE) 

XGBoost <- function(train, tuneLength){
  
  set.seed(100)                                         # seed 고정 For Cross Validation
  caret.xgb <- caret::train(y~., data = train, 
                            method = "xgbTree",          
                            trControl = fitControl,
                            # objective = "reg:squarederror", # error(The following parameters were provided multiple times)
                            tuneLength = tuneLength     # tuneLength (탐색할 후보 모수 갯수)
  )   
  
  return(caret.xgb)
  
}

XGB.Caret <- XGBoost(Train.Data, 2)
XGB.Caret
XGB.Caret$finalModel
XGB.Caret$finalModel$tuneValue
```

----------------- 

### **예측**

```{r}
# Final Prediction (Detrended + Trend)
Pred.XGB  <- predict(XGB.Caret, Test.X) + trend.arima.pred$mean
```

----------------- 

### **Accuracy**

```{r}
# Accuracy
acc_XGB   <- accuracy(c(Pred.XGB), test.ts)
acc_XGB
```

----------------- 

### **예측 그래프**

```{r}
# Prediction Plot
ggplot(tail(Amtrak.data, n.test), aes(x = Month)) +
  geom_line(aes(y = Ridership, colour = "Observation", linetype = "Observation")) + 
  geom_line(aes(y = Pred.XGB, colour = "Prediction", linetype = "Prediction")) +
  scale_color_manual(name = "", values = c("Observation" = "black", "Prediction" = "blue")) +
  scale_linetype_manual("", values = c("Observation"= 1, "Prediction" = 2)) +
  theme_bw()
```

-----------------

## **4. Stacking**

- 앙상블의 종류 중 하나로, 개별적인 알고리즘을 통해 생성된 예측 결과들로 새로운 Training Dataset을 만들어 최종 모형을 적합시킨다.


------------------

### **모형 적합**

```{r}
# Ref. https://cran.r-project.org/web/packages/caretEnsemble/vignettes/caretEnsemble-intro.html
#      https://github.com/zachmayer/caretEnsemble/blob/master/R/caretStack.R


# 1. Modeling for Stacking (Declare Individual Model)
set.seed(100)
fitControl <- trainControl(method = "repeatedcv",        # adaptive_cv
                           number = 5,
                           repeats = 5,
                           # adaptive = list(min = 5,
                           #                 alpha = 0.05,
                           #                 method = "BT",
                           #                 complete = TRUE),
                           # search = "random",            # grid
                           savePredictions = "final",      # 최적 모수에 대한 예측 저장
                           # classProbs = TRUE,            # 각 클래스에 대한 확률 저장(Classification)
                           index = createResample(Train.Data$y, 1),  # index : 각 resapling에 대한 요소, Training에 사용되는 행 번호/ createResample : 붓스트랩
                           allowParallel = TRUE) 


# 원본 Training Data에 학습시킨 Hyperparameter 결과
alg_tune_list <- list(                            # Do not use custom names in list. Will give prediction error with greedy ensemble. Bug in caret.
  parRF = caretModelSpec(method="parRF",
                         importance = TRUE,
                         nodeside = 5,
                         tuneGrid = expand.grid(mtry=RF.Caret$finalModel$tuneValue$mtry)),
  xgbTree = caretModelSpec(method="xgbTree",
                           tuneGrid = expand.grid(nrounds = XGB.Caret$finalModel$tuneValue$nrounds,
                                                  max_depth = XGB.Caret$finalModel$tuneValue$max_depth,
                                                  eta = XGB.Caret$finalModel$tuneValue$eta,
                                                  gamma = XGB.Caret$finalModel$tuneValue$gamma,
                                                  colsample_bytree = XGB.Caret$finalModel$tuneValue$colsample_bytree,
                                                  min_child_weight = XGB.Caret$finalModel$tuneValue$min_child_weight,
                                                  subsample = XGB.Caret$finalModel$tuneValue$subsample)))

set.seed(100)
multi_mod <- caretList(y~., data = Train.Data, trControl = fitControl, 
                       tuneList = alg_tune_list)  # search = "grid"     


multi_mod$parRF
multi_mod$xgbTree

# 2. Stacking (개별 모형들의 예측값을 결합한 Data를 Training data로 쓰는 Final Model)

set.seed(100)
stackControl <- trainControl(method = "adaptive_cv",
                             number = 5,
                             repeats = 5,
                             adaptive = list(min = 5,
                                             alpha = 0.05,
                                             method = "BT",
                                             complete = TRUE),
                             search = "random",
                             allowParallel = TRUE) 

set.seed(100)
stack.xgb <- caretStack(multi_mod, method = "xgbTree",  # Final Model
                        trControl = stackControl, 
                        tuneLength = 2)                 # 모수 후보 갯수

stack.xgb
stack.xgb$ens_model$finalModel
stack.xgb$ens_model$finalModel$tuneValue
```

----------------- 

### **예측**

```{r}
# Final Prediction (Detrended + Trend)
stack.Pred.XGB <- predict(stack.xgb, Test.X) + trend.arima.pred$mean
```

----------------- 

### **Accuracy**

```{r}
# Accuracy
acc_stack.XGB  <- accuracy(c(stack.Pred.XGB), test.ts)
acc_stack.XGB
```

----------------- 

### **예측 그래프**

```{r}
# Prediction Plot
ggplot(tail(Amtrak.data, n.test), aes(x = Month)) +
  geom_line(aes(y = Ridership, colour = "Observation", linetype = "Observation")) + 
  geom_line(aes(y = stack.Pred.XGB, colour = "Prediction", linetype = "Prediction")) +
  scale_color_manual(name = "", values = c("Observation" = "black", "Prediction" = "blue")) +
  scale_linetype_manual("", values = c("Observation"= 1, "Prediction" = 2)) +
  theme_bw()
```

-----------------

## **5. Stacking with GLM**

- 최종 모형에 Generalized Linear Model(GLM)을 사용함으로써, 개별 모형들의 예측 결과들에 가중치를 곱하여 더한 것을 최종 예측으로 생성한다.

------------------

### **모형 적합**

```{r}
# Ref. https://github.com/zachmayer/caretEnsemble/blob/master/R/caretEnsemble.R

set.seed(100)
greedyEnsemble <- caretEnsemble(multi_mod, trControl = trainControl(method = "cv", number=5))
greedyEnsemble
greedyEnsemble$ens_model$finalModel
summary(greedyEnsemble)
```

----------------- 

### **예측**

```{r}
# Final Prediction (Detrended + Trend)
stack.Pred.GLM <- predict(greedyEnsemble, Test.X) + trend.arima.pred$mean
```

----------------- 

### **Accuracy**

```{r}
# Accuracy
acc_stack.GLM <- accuracy(c(stack.Pred.GLM), test.ts)
acc_stack.GLM
```

----------------- 

### **예측 그래프**

```{r}
# Prediction Plot
ggplot(tail(Amtrak.data, n.test), aes(x = Month)) +
  geom_line(aes(y = Ridership, colour = "Observation", linetype = "Observation")) + 
  geom_line(aes(y = stack.Pred.GLM, colour = "Prediction", linetype = "Prediction")) +
  scale_color_manual(name = "", values = c("Observation" = "black", "Prediction" = "blue")) +
  scale_linetype_manual("", values = c("Observation"= 1, "Prediction" = 2)) +
  theme_bw()
```

-----------------

## **예측 비교2**

### **Line Plot**

```{r}
# 1. Line Plot
Pred.Data3 <- data.frame("Date" = tail(Amtrak.data$Month, n.test),
                         "Observation" = test.ts,
                         "NNAR" = NNAR.forecast,
                         "RF" = c(Pred.RF),
                         "XGBoost" = c(Pred.XGB),
                         "Stack.XGBoost" = c(stack.Pred.XGB),
                         "Stack.GLM" = c(stack.Pred.GLM))


Pred.Models3 <- reshape2::melt(Pred.Data3, id.vars="Date",
                               variable.name="Type",
                               value.name='y')

models2 <- unique(Pred.Models3$Type)

lcols <- c("#000000", "#66CC99", "#ae5d8b", "#c1c084", "#d38b72", "#dc143c", "#00498c", "#9999CC")

ggplot(Pred.Models3, aes(x=Date, y=y, group=Type)) +
  geom_line(aes(color=Type), size=1, linetype="solid") +
  labs(x="Date", y="Ridership") +                # y="log(Charging demand)"
  scale_color_manual(values = lcols, name = 'Models', labels = models2) +
  #scale_x_datetime(date_breaks= "24 hour", date_labels = paste("%Y- %m-%d %H", "h", sep="")) +  # %Y- %m-%d %H:%M:%S"
  theme_bw() +
  theme(axis.text.y=element_text(size=16),
        axis.title=element_text(size=20),
        axis.text.x=element_text(angle=30, hjust=1, size=12),
        legend.title = element_text(size=18),
        legend.text = element_text(size=16))  

```

-----------------

### **Scatter Plot**


```{r}
# 2. Scatter Plot
Pred.Models4 <- reshape2::melt(Pred.Data3[,-1], id.vars="Observation",
                               variable.name="Type",
                               value.name='Pred')

ggplot(Pred.Models4, aes(x=Observation, y=Pred)) + 
  geom_point(color="#d38b72") +
  geom_abline(intercept = 0, slope = 1, size=1, colour="#9999CC", linetype="dashed") +
  facet_wrap( ~ Type, ncol=2) +
  labs(x="Observation", y="Prediction", color = "") +
  theme_bw() +
  theme(axis.title=element_text(size=20),
        axis.text=element_text(size=15),
        strip.text.x = element_text(size=18, face="bold")) 
```

