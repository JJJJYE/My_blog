---
title: "Principal Component Analysis"
description: |
  Description for Principal Component Analysis
author:
  - name: Yeongeun Jeon
date: 10-23-2022
preview: prin.PNG
categories: Multivariate Data Analysis
output: 
  distill::distill_article:
        toc: TRUE
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, warning = FALSE, message = FALSE)
options(width=200)
```


```{css, echo=FALSE}

p, ul, li{
text-align: justify
}

```

- **참고**
    - **SAS를 이용한 다변량 통계 분석, 김재희 저**
    - **R을 활용한 다변량 자료분석 방법론, 강현철 $\cdot$ 연규필 $\cdot$ 한상태 저**

# **1. 서론**

- 통계적 처리과정에서 다변량 데이터 분석의 중요한 목적 중 하나는 여러 변수들의 정보를 일차원 또는 소수 몇 개의 차원으로 축소함으로서, 복잡한 다차원 구조를 단순화시켜 보다 쉽게 이해하려는 데 있다.
- 여기에서 다룰 주성분 분석(Principal Component Analysis)은 여러 개의 변수로 얻어진 다변량 데이터에 대해, 분산-공분산 구조를 `변수들의 선형결합식(주성분 : Principal Component)`으로 설명하고자 하는 접근방법이다.
    - 변수들의 선형결합식 또는 선형변환되어 형성된 주성분은 `서로 독립`적인 새로운 변수이며, $p$개의 변수에 포함된 데이터의 변동을 $m(\le p)$개의 주성분으로 대신하여 설명한다.
- 주성분 분석의 목적은 다음과 같다.
    1. `차원 축소`
        - 다변량 데이터의 변수 개수($p$)가 너무 많기 때문에 `서로 독립`적인 새로운 변수 개수 $m(<p)$으로 축소
    2. 변동이 큰 축 탐색
    3. 주성분을 통한 데이터의 해석
        - 새롭게 생성된 $m$개의 변수들에 대한 해석
- 어떤 다변량 데이터에 대해, 예를 들어 2개의 주성분으로 전체 변동의 대부분을 설명하도록 변환할 수 있다면 `정보의 손실을 최소화, 즉, 가능한 많은 정보(변동, Variation)를 보존`하면서 저차원 공간에서 데이터를 해석할 수 있게 되어 축약 뿐만 아니라 해석에 대한 이점도 얻을 수 있다.   
- 주성분 분석은 여러 통계분석 기법들과 많은 이론적 관련성을 지니고 있어 넓은 응용의 영역을 가진다.
    - 특히 `차원축소`의 결과로부터 얻어지는 주성분점수들은 또 다른 통계분석들(예를 들어, 군집분석, 인자분석 등)을 위한 입력자료로 이용되어 일련의 분석과정에서 하나의 중간단계의 역할을 하기도 한다.

------------------------

# **2. 선형변환과 주성분**

- 예를 들어, 다음 표에서와 같이 5명의 학생으로부터 국어($x_1$), 수학($x_2$), 영어($x_3$) 등 3개 과목의 점수를 얻었다고 하면, 이 경우 학생들의 성적을 평가하기 위해 일반적으로 사용되는 방법 중 하나는 각 학생들에 대해 3과목의 평균점수를 계산하고 비교하는 것이다.
    - 즉, 3과목의 점수를 일일이 관찰하여 학생들의 순위를 파악하는 대신 평균점수로 각 학생들의 점수를 요약하고, 이 값들의 순위를 통해 학생들의 성적을 평가하고자 하는 것이다.
    - 평균점수는 원래 3차원의 변수들을 선형변환시켜 1차원의 변수로 요약한 것이라 할 수 있다.
        
<center>
![](그림1.png){width=80%}
</center>
</br>  
    
- $p\times 1$ 확률벡터 $\mathbf{X}=(X_1, \ldots, X_p)^T$에 대해 다음과 같은 형태의 가중결합을 취한 것을 선형변환(Linear Transformation)이라고 한다.
$$
\begin{align*}
Y=w_1X_1+w_2X_2 + \ldots + w_pX_p.
\end{align*}
$$
- 이때 변환값 $Y$는 가중계수(Weight)들을 모아놓은 벡터 $\boldsymbol{w}=(w_1, \ldots, w_p)^T$의 적절한 선택에 따라 원래의 $\mathbf{X}$보다 더 유용한 정보를 보유할 수도 있고, 이를 통한 차원 축소는 물론 변수들간의 관계를 단순화시키는 데에도 큰 역할을 한다.
    - 위의 표에서 평균점수는 가중계수가 모두 동일한 1/3이다.
- 이와 같은 선형결합 형태의 변환은 $p$-차원의 정보를 선형적인 개념에서 `1-차원으로 축소`하는 것이라고 할 수 있는데, 이때 `가중계수들을 어떻게 선택하느냐`가 중요한 문제가 된다.
- 주성분 분석은 $p$-차원 공간에 흐뜨러져 있는 점들을 직교최소제곱(Orthogonal Least Square)의 개념에 따라 가장 잘 적합시키는 직선(또는 평면)을 찾기 위한 기하적 최적화의 문제로 제기되었다. 
- 여러 학자들은 변수들 간의 상관구조를 분석하기 위해 원래 변수들의 변동을 결정하는 보다 낮은 차원의 서로 독립적인 요인을 상정하여 이를 성분(Component)이라 하였으며, 원래 변수들이 가진 전체 변동(분산)에 대한 성분의 공헌도(설명력)를 순차적으로 최대화하도록 성분들을 유도하여 이에 기초한 분석을 주성분 분석이라고 불렀다.

-------------------------

# **3. 개념과 정의**

## **3-1. 개념**

- $p\times 1$ 확률벡터 $\mathbf{X}=(X_1, \ldots, X_p)^T$가 모평균벡터 $\boldsymbol{\mu}$와 모공분산행렬 $\boldsymbol{\Sigma}$를 가진다고 할 때, 이러한 확률벡터는 수학적으로 $p$-차원에 놓이게 되며 원래 변수의 선형결합 또는 회전 변환을 통해 $p$개의 새로운 좌표축을 형성할 수 있다.
- 이때 생기는 새로운 축은 `데이터의 변동을 최대로 설명`해주는 동시에 공분산 구조에 대한 해석을 용이하게 하도록 만들어질 수 있는데 이것을 주성분이라고 한다.
    - 첫 번째 주성분(제1 주성분)은 데이터의 변동을 최대로 설명해주는 변수들의 선형결합식이다.
    - 두 번째 주성분(제2 주성분)은 첫 번째 주성분 다음으로 변동을 가장 많이 설명해주는 변수들의 선형결합식이며 `첫 번째 주성분과는 독립`이다.
    - 이와 같이 찾아진 $p$개의 주성분들은 새로운 축을 형성하며, 주성분과 이들이 설명하는 변동량, 주성분, 주성분점수 등은 변수들로 표현된 시스템에 대한 이해를 돕는다.
    
<center>
![](prin.png){width=100%}
</center>    
    
-----------------------------

## **3-2. 정의**

- 확률벡터 $\mathbf{X}=(X_1, \ldots, X_p)^T$의 공분산행렬 $\boldsymbol{\Sigma}$은 고유값 $\lambda_1\ge\lambda_2\ge\ldots\lambda_p\ge 0$을 갖고 각 고유값에 해당하는 단위 고유벡터 $\boldsymbol{e}_1, \boldsymbol{e}_2, \ldots, \boldsymbol{e}_p$를 가진다.
    - 즉, 고유값-고유벡터를 짝지으면 $(\lambda_1, \boldsymbol{e}_1), (\lambda_2, \boldsymbol{e}_2), \ldots, (\lambda_p, \boldsymbol{e}_p)$이다.
- 다음과 같은 $p$개의 선형결합식을 생각해보자.
$$
\begin{align*}
Y_1 &= \boldsymbol{l}^T_1\mathbf{X}=l_{11}X_1+\ldots +l_{p1}X_p,\\
Y_2 &= \boldsymbol{l}^T_2\mathbf{X}=l_{12}X_1+\ldots +l_{p2}X_p,\\
&\vdots\\
Y_p &= \boldsymbol{l}^T_p\mathbf{X}=l_{1p}X_1+\ldots +l_{pp}X_p.\\
\end{align*}
$$
    - 여기서 $\boldsymbol{l}^T_i = (l_{1i}, \ldots, l_{pi})$이며 선형결합식들의 분산과 공분산은
$$
\begin{align*}
Var(Y_i)&=\boldsymbol{l}^T_i \boldsymbol{\Sigma}\boldsymbol{l}_i, \;\;i = 1, \ldots, p,\\
Cov(Y_i, Y_k)&=\boldsymbol{l}^T_i \boldsymbol{\Sigma}\boldsymbol{l}_k, \;\;i,k = 1, \ldots, p
\end{align*}
$$
이 된다.
- 주성분은 아래의 과정을 통해 구할 수 있다.
    (1) 첫 번째 주성분은 $\boldsymbol{l}^T_1\boldsymbol{l}_1 = 1$을 만족하는 $p\times 1$ 벡터 $\boldsymbol{l}_1$에 대해 $Var(\boldsymbol{l}^T_1\mathbf{X})$를 최대로 하는 선형결합식 $\boldsymbol{l}^T_1\mathbf{X}$로 구한다.
    (2) 두 번째 주성분은 $\boldsymbol{l}^T_2\boldsymbol{l}_2 = 1$을 만족하는 $p\times 1$ 벡터 $\boldsymbol{l}_2$에 대해 $Var(\boldsymbol{l}^T_2\mathbf{X})$를 최대로 하며 또한 $Cov(\boldsymbol{l}^T_1\mathbf{X}, \boldsymbol{l}^T_2\mathbf{X})=0$인 선형결합식 $\boldsymbol{l}^T_2\mathbf{X}$로 구한다.
    (3) $i$번째 주성분은 $\boldsymbol{l}^T_i\boldsymbol{l}_i = 1$을 만족하는 $p\times 1$ 벡터 $\boldsymbol{l}_i$에 대해 $Var(\boldsymbol{l}^T_i\mathbf{X})$를 최대로 하며 앞에서 구한 ($i-1$)개의 주성분들과는 직교하도록 $Cov(\boldsymbol{l}^T_i\mathbf{X}, \boldsymbol{l}^T_j\mathbf{X})=0,\; j=1, \ldots, i-1$인 선형결합식 $\boldsymbol{l}^T_i\mathbf{X}$로 구한다.
    (4) 마지막으로 $p$번째 주성분은 $\boldsymbol{l}^T_p\boldsymbol{l}_p = 1$을 만족하는 $p\times 1$ 벡터 $\boldsymbol{l}_p$에 대해 $Var(\boldsymbol{l}^T_p\mathbf{X})$를 최대로 하며 앞에서 구한 ($p-1$)개의 주성분들과는 직교하도록 $Cov(\boldsymbol{l}^T_p\mathbf{X}, \boldsymbol{l}^T_j\mathbf{X})=0,\; j=1, \ldots, p-1$인 선형결합식 $\boldsymbol{l}^T_p\mathbf{X}$로 구한다.
- 이와 같이 정의된 주성분을 구하기 위해 수학적으로 Lagrange 기법을 이용하면 $i$번째 주성분의 가중계수 $\boldsymbol{l}_i$는 $i$번째 고유값에 대응되는 단위 고유벡터 $\boldsymbol{e}_i$가 된다.
- 게다가, $i$번째 주성분이 설명하는 데이터 변동의 양은 $i$번째 고유값으로 나타난다.
    - 즉, $Var(\boldsymbol{l}^T_i\mathbf{X})=\lambda_i$.
    - $i$번째 주성분에 의해 설명되는 전체 분산의 비율은 다음과 같다.
$$
\begin{align*}
\frac{\lambda_i}{\lambda_1+\ldots+\lambda_p},\; i=1, \ldots, p.
\end{align*}
$$   
        - 여기서 분자 $\lambda_i$는 $i$번째 주성분에 의해 설명되는 분산, 분모 $\lambda_1+\ldots+\lambda_p$는 전체 분산을 의미한다.
            - 데이터의 전체 분산은 공분산행렬 $\boldsymbol{\Sigma}$의 대각성분의 합, 즉, $\sum_{i=1}^p Var(X_i)=tr(\boldsymbol{\Sigma})=\sigma^2_1+\ldots+\sigma^2_p=\lambda_1+\ldots+\lambda_p=\sum_{i=1}^p Var(Y_i)$이다.
- 요약하자면, `공분산행렬` $\boldsymbol{\Sigma}$`의 고유값과 단위 고유벡터는 각각 주성분이 설명하는 데이터 변동의 양과 주성분의 가중계수`가 된다.
- $i$ 번째 주성분 $Y_i$과 $X_k$(원래 데이터의 $k$번째 확률변수)의 상관계수 $\rho_{Y_i, X_k}$는 다음과 같다.
$$
\begin{align}
\rho_{Y_i, X_k}=\frac{e_{ki}\sqrt{\lambda_i}}{\sqrt{\sigma^2_k}}, \; k=1,\ldots, p.
\end{align}
$$
    - 여기서 $e_{ki}$는 $\boldsymbol{e}_i$의 $k$번째 성분이다.

--------------------

# **4. 상관행렬을 이용한 주성분 분석**

- 일반적으로 관찰된 데이터의 의미를 해석하려면 표본공분산행렬 $\boldsymbol{S}$를 이용해 주성분 분석을 한다.
- 그러나, 변수의 단위가 다르거나 또는 분산의 차이가 큰 경우, 표본공분산행렬 $\boldsymbol{S}$ 대신 표본상관행렬 $\boldsymbol{R}$을 이용해 주성분을 구하면, 해석하는데 편리한 이점이 있다.
- 표본공분산행렬 $\boldsymbol{S}$을 사용할 경우, 분산이 큰 변수가 주성분의 압도적인 비중을 차지할 수 있으므로 분석의 균형을 유지하기 위해서 표본상관행렬 $\boldsymbol{R}$을 이용할 필요가 있다.
- 주성분의 가중계수는 척도변환에 따라 변하므로, 공분산행렬을 이용한 경우의 주성분 결과와 상관행렬을 이용해 얻은 주성분 결과에 차이가 나며, 설명하는 변동의 비율도 차이가 난다.
- 주성분 분석에서 표본상관행렬 $\boldsymbol{R}$을 사용할 경우 주의해야할 점은 다음과 같다.
    1. $\boldsymbol{R}$에 의한 주성분과 $\boldsymbol{S}$에 의한 주성분이 설명하는 분산의 양이 다르다.
    2. $\boldsymbol{R}$에 의한 주성분과 $\boldsymbol{S}$에 의한 주성분 계수가 다르다.
    3. $\boldsymbol{R}$ 자체가 척도 불변이므로 $\boldsymbol{R}$에 의한 주성분은 척도 불변이다.
    4. $\boldsymbol{R}$에 의한 주성분은 유일하지 않다.


-------------------------

## **4-1. 정의**

- 다음과 같은 표준화 변수들로 구성한 벡터 $\mathbf{Z}=(Z_1, \ldots, Z_p)^T$의 평균벡터는 $\boldsymbol{\mu}=\mathbf{0}$이고 (표본)공분산행렬은 원 데이터의 (표본)상관행렬 $\boldsymbol{R}$과 같다.
$$
\begin{align*}
Z_1 &= \frac{X_1-\mu_1}{\sqrt{\sigma^2_1}},\\
Z_2 &= \frac{X_2-\mu_2}{\sqrt{\sigma^2_2}},\\
&\vdots\\
Z_p &= \frac{X_p-\mu_p}{\sqrt{\sigma^2_p}}.\\
\end{align*}
$$
- 표준화 변수들로 구성한 벡터 $\mathbf{Z}=(Z_1, \ldots, Z_p)^T$의 공분산행렬 $\boldsymbol{R}$은 고유값 $\lambda_1\ge\lambda_2\ge\ldots\lambda_p\ge 0$을 갖고 각 고유값에 해당하는 단위 고유벡터 $\boldsymbol{e}_1, \boldsymbol{e}_2, \ldots, \boldsymbol{e}_p$를 가진다.
    - 여기서 표준화 변수의 표본공분산행렬은 표본상관행렬이기 때문에 상관행렬을 이용한 주성분 분석은 다음과 같은 표준화 변수들에 대한 주성분 분석과 같다.
- 앞에서 정리한 [주성분 정의][**3-2. 정의**]를 이용하면 $i$번째 주성분 $Y_i$는
$$
\begin{align*}
Y_i = \boldsymbol{e}^T_i\mathbf{Z}=e_{1i}Z_1+\ldots + e_{pi}Z_p,\;\; i = 1, \ldots, p
\end{align*}
$$
가 된다.
    - 여기서 $\boldsymbol{e}^T_i$는 표본상관행렬 $\boldsymbol{R}$의 $i$번째 고유값에 대응되는 단위 고유벡터이다.
- 주성분에 대한 전체 변동(분산)은 $\sum_{i=1}^pVar(Y_i)= \sum_{i=1}^{p} Var(\boldsymbol{e}^T_i\mathbf{Z})=\sum_{i=1}^{p} Var(Z_i)=p$로 변수 개수와 같다.
- $i$번째 주성분 $Y_i$와 $k$번째 표분화 변수 $Z_k$의 상관계수는 $\rho_{Y_i, Z_k}$는 다음과 같다.
$$
\begin{align}
\rho_{Y_i, Z_k}=\frac{e_{ki}\sqrt{\lambda_i}}{\sqrt{Var(Z_k)}} = e_{ki}\sqrt{\lambda_i}, \; k=1,\ldots, p.
\end{align}
$$
    - 여기서 $e_{ki}$는 표본상관행렬 $\boldsymbol{R}$의 $i$번째 고유값에 대응되는 단위 고유벡터 $\boldsymbol{e}_i$의 $k$번째 성분이다.
- 또한, 표준화 변수로부터 구한 주성분에 대하여 $i$번째 주성분에 의해 설명되는 전체변동(분산)의 비율은
$$
\begin{align*}
\frac{\lambda_i}{p},\; i=1, \ldots, p.
\end{align*}
$$  
이다.
    - 여기서 분자 $\lambda_i$는 $i$번째 주성분에 의해 설명되는 분산, 분모 $p$는 전체 분산을 의미한다.
        - 전체 분산은 $\boldsymbol{R}$의 대각성분의 합, 즉, $tr(\boldsymbol{R})=1+\ldots+1=p$이다.

----------------------

# **5. 주성분 그래프**

- 2차원 좌표축에 표현할 수 있는 첫 번째 주성분과 두 번째 주성분은 원래 데이터에 대한 중요한 특성을 나타낼 수 있다.
- 따라서, 주성분 그래프(Principal Component Graph)로부터 두 주성분간의 관계와 패턴을 도출할 수 있으며, 또한 전체 데이터가 주성분을 통해 변화되어 나타내는 관계도 알 수 있다.
- 처음 몇 개의 주성분은 분산 또는 공분산 구조를 왜곡시키는 이상점(Outlier)의 영향을 많이 받는다.
- 그리고, 마지막 몇 개의 주성분은 인공적인 차원을 유도하는 이상값에 의해 민감하게 변한다.

----------------------

# **6. 주성분 개수 선택**

- 주성분 분석 시 원래 데이터에 대한 정보의 손실을 최소화하기 위해서는 적절한 개수의 주성분을 선택해야 한다.
    - $p$개의 변수로 인해 $p$개의 주성분이 나왔다고 모두 다 사용한다면 `차원 축소`가 목적인 주성분 분석의 의미가 없어진다.
- 데이터의 전체 변동을 대부분 설명하기 위하여 설정해야 할 주성분의 개수를 결정하는 몇 가지 기준들에 대해 설명하기로 한다.

--------------------------

## **6-1. 전체 변동에서의 공헌도**

- 전체 변동의 70~90%가 되도록 주성분의 수를 결정한다.
- $i$번째 주성분이 설명하는 분산의 양을 $\lambda_i$라고 하면, $m$개의 주성분에 의해 설명되는 전체 변동의 비율은 다음과 같다.
$$
\begin{align*}
\frac{\sum_{i=1}^m \lambda_i}{\text{전체 분산}}.
\end{align*}
$$  
    - 표본공분산행렬을 이용할 경우, 분모인 전체 분산은 $\lambda_1+\ldots+\lambda_p$이 되며, $\lambda_i$는 표본공분산행렬의 $i$번째 고유값이다.
    - 표본상관행렬을 이용할 경우, 분모인 전체 분산은 변수 개수 $p$가 되며, $\lambda_i$는 표본상관행렬의 $i$번째 고유값이다.
- $m$개의 주성분에 의해 설명되는 전체 변동의 비율인 위의 식이 0.7~0.9가 되는 $m$으로 정한다.

----------------------------

## **6-2. 평균 고유값**

- 고유값들의 평균 $\bar{\lambda}=\frac{1}{p}\sum_{i=1}^p \lambda_i$을 구한 후 고유값이 평균값 이상이 되는 주성분을 선택한다.
- 상관행렬을 사용한 경우, 평균 고유값은 1이 된다.
    - $\lambda_1+\ldots+\lambda_p=\sigma^2_1+\ldots+\sigma^2_p=1+\ldots+1=p.$

------------------------------

## **6-3. 스크리 그래프**

- 2차원 좌표축에 (고유값 순서, 고유값 크기)로 ($i,\; \lambda_i$) 점을 찍고 점간을 선분으로 연결한 그림을 스크리 그래프(Scree Graph)라고 한다.
- 값이 큰 고유값부터 크기 순으로 점을 찍을 때 값의 차이가 크면 가파른 경사가 되고 고유값의 변화가 작으면 내리막 경사가 완만해진다.
- 가파른 정도를 보고 큰 고유값과 작은 고유값을 구분하여 자연스럽게 적절한 개수를 정한다.
    - 큰 고유값은 주성분의 분산이 큰 것을 의미하기 때문에 해당 주성분에 의해 설명되는 전체 변동의 비율이 크다는 것을 의미한다.
- 다음 그림을 보면 두 개의 고유값이 크며 가파른 경사를 형성하며, 나머지 고유값들은 기울기의 변화가 작고 완만한 경사를 형성하므로 앞의 두 주성분의 비중이 큼을 알 수 있다.


![출처 : https://www.janda.org/workshop/factor%20analysis/SPSS%20run/SPSS08.htm](scree.gif){width=80%}


- 그러나, 그래프로 명확한 구별이 어려울 때는 [6-1. 전체 변동에서의 공헌도][**6-1. 전체 변동에서의 공헌도**]와 [6-2. 평균 고유값][**6-2. 평균 고유값**]의 방법을 동시에 고려하여 적절한 주성분 개수를 선택해야 한다.


--------------------------

# **7. 예제**

- 주성분 분석을 수행하기 위해 R에 내장되어 있는 함수 `prcomp()`와 `princomp()`를 이용할 수 있다.
- 입력 데이터로는 데이터행렬, 공분산행렬, 상관행렬, 상관행렬과 표준편차 등이 사용될 수 있다.

--------------------------

## **7-1. 고객만족 데이터**

- [자유아카데미](http://www.freeaca.com/new/library/BoardFileList.aspx?page=1&sword=%eb%8b%a4%eb%b3%80%eb%9f%89&stype=title&area=2)에서 출판한 책 **R을 활용한 다변량 자료분석 방법론**의 데이터 파일 중 "satis.csv"를 활용한다.
- 이 데이터는 어떤 제품에 대한 고객의 만족도를 조사하여 얻어진 데이터로 총 8개의 변수들로 이루어져 있다.
    1. ID : 고객 아이디
    2. gender : 고객 성별
    3. age : 고객 나이
    4. $x_1$ : 가격에 대한 만족도
    5. $x_2$ : 성능에 대한 만족도
    6. $x_3$ : 편리성에 대한 만족도
    7. $x_4$ : 디자인에 대한 만족도
    8. $x_5$ : 색상에 대한 만족도
- 만족도는 5점 척도로 측정되어 있으며, "1 = 매우 만족하지 않는다.", "2 = 만족하지 않는다.", "3 = 보통이다.", "4 = 만족한다.", "5 = 매우 만족한다."를 의미한다.  

```{r}
# 데이터 불러오기
satis <- read.csv("C:/Users/User/Desktop/satis.csv")
satis
```
  
  
----------------------------

```{r}
# 기술통계량 출력
pacman::p_load("psych")
describe(satis[,c("x1", "x2", "x3", "x4", "x5")])
```

`Caution!` Package `psych`에 내장되어 있는 함수 `describe()`를 이용하여 기술통계량을 출력할 수 있다.  
`Result!` 변수 $x_1$의 표준편차는 다른 변수들에 비해 상대적으로 크지만, 변수 $x_3$는 다른 변수들에 비해 상대적으로 표준편차가 작다.

-----------------

```{r}
# 상관행렬 출력
cor(satis[,c("x1", "x2", "x3", "x4", "x5")])
```

`Result!` 상대적으로 표준편차가 큰 변수 $x_1$은 다른 변수들과 상관계수도 높으며, 특히, 변수 $x_3$와 큰 양의 상관성을 가지고 있다. 반면, 상대적으로 표준편차가 작은 변수 $x_3$는 변수 $x_4$, $x_5$와의 상관계수가 낮다. 그리고, 변수 $x_4$와 $x_5$ 사이에는 매우 큰 양의 상관성을 가지고 있다($r=0.94$). 

-----------------

```{r}
# 공분산행렬을 이용한 주성분 분석
satis.prcomp <- prcomp(satis[,c("x1", "x2", "x3", "x4", "x5")])
satis.prcomp
```

`Caution!` 주성분 분석은 R에 내장되어 있는 함수 `prcomp()`를 통해 수행할 수 있다. 자세한 옵션은 `?prcomp`를 통해 확인한다.  
`Result!` 함수 `prcomp()`로부터 얻어진 리스트 객체에는 각 결과들이 별도로 저장되어 있다. 첫 번째 결과 "Standard deviations"에는 주성분의 표준편차(즉, 공분산행렬의 고유값의 제곱근)이 저장되어 있다. 두 번째 결과 "Rotation"에는 주성분 계수(즉, 공분산행렬의 단위 고유벡터)가 저장되어 있다.

----------------

```{r}
satis.prcomp$sdev        # 주성분의 표준편차
satis.prcomp$sdev^2      # 주성분의 분산 = 주성분에 의해 설명되는 분산의 양 = 공분산행렬의 고유값
```

`Caution!` 함수 `eigen(cov(satis[,c("x1", "x2", "x3", "x4", "x5")]))`를 통해 주성분의 분산이 공분산행렬의 고유값임을 확인할 수 있으며, 공분산행렬의 대각성분 합이 주성분 분산의 합(공분산행렬의 고유값의 합)과 같음을 알 수 있다.

----------------

```{r}
# 주성분 계수
satis.prcomp$rotation    
```

`Caution!` 함수 `eigen(cov(satis[,c("x1", "x2", "x3", "x4", "x5")]))`를 통해 주성분 계수가 공분산행렬의 단위 고유벡터임을 확인할 수 있다.  
`Result!` 주성분 계수에 의하면, 첫 번째 주성분 $Y_1=0.572C_{1}+0.397C_{2}+0.256C_{3}+0.462C_{4}+0.486C_5$이며, 두 번째 주성분 $Y_2=0.292C_1+0.600C_2+0.223C_3-0.598C_4-0.383C_5$이 된다. 여기서 확률변수 $C_{i}$는 $X_i-\mu_i$이며, $\mu_i$는 $X_i$의 평균을 의미한다. 즉, 함수 `prcomp()`는 기본적으로 중심화(평균이 0)된 데이터행렬을 대상으로 한다. 즉, 위의 분석 결과는 `prcomp(satis[,c("x1", "x2", "x3", "x4", "x5")], center = TRUE, scale = FALSE)`와 동일하다.  
주성분의 의미를 해석해보면, 첫 번째 주성분은 주성분 계수가 서로 비슷하여 전반적인 만족도를 나타낸다고 할 수 있으며, 두 번째 주성분은 변수 $x_1$, $x_2$, $x_3$와 변수 $x_4$, $x_5$의 부호가 반대이므로 가격, 성능, 편리성 제품의 "내형적 요인"과 디자인, 색상 등 제품의 "외형적 요인"의 만족도 차이를 나타낸다고 할 수 있다.

----------------

```{r}
# 요약
summary(satis.prcomp)    
```

`Result!` 첫 2개의 고유값은 5.082, 2.368로서 각각 전체 변동의 62.6%(=5.082/8.122)와 29.2%(=2.368/8.122)를 차지한다. 따라서, 첫 2개의 주성분에 의해 데이터 변동의 약 91.7%가 설명될 수 있음을 알 수 있다. 이는 2개의 주성분으로 데이터 변동을 충분히 설명할 수 있음을 가리키며, 만약 첫 2개의 주성분을 취할 경우 이는 원래 다섯 개의 관찰변수들이 가지고 있는 정보를 2차원으로 축약한다는 의미를 가진다. 

------------------

```{r}
# 스크리 그래프
par(mfrow = c(1, 2))
screeplot(satis.prcomp, type = "b", main = "")  # 막대그래프
screeplot(satis.prcomp, type = "l", main = "")  # 선 그래프
```

`Caution!` 함수 `screeplot()`을 이용하여 스크리 그래프를 작성할 수 있다.  
`Result!` 스트리 그래프를 보면 첫 번째 주성분과 두 번째 주성분의 분산(고유값)이 크며 가파른 경사를 형성한다. 그러므로, 주성분 개수는 2개로 선정할 수 있다.

------------------

```{r}
# 주성분점수
satis.score <- satis.prcomp$x           
satis.score
```

`Caution!` 주성분점수는 객체의 관찰값에 의해서 얻어지는 주성분 값으로, 다음 단계의 통계분석(예컨대, 회귀분석 또는 군집분석)에서 원래의 관찰변수 대신에 유용하게 사용될 수 있다. 함수 `prcomp()`로부터 얻어진 리스트 객체에는 주성분점수도 포함되어 있으며, 이는 객체 "x"에 저장되어 있다.  
`Result!` 첫 번째 주성분과 두 번째 주성분에 의해 얻어지는 $i$번째 개체의 주성분점수는 다음과 같다.
$$
\begin{align}
\begin{cases}
y_{i1}=0.572c_{i1}+0.397c_{i2}+0.256c_{i3}+0.462c_{i4}+0.486c_{i5},\\
y_{i2}=0.292c_{i1}+0.600c_{i2}+0.223c_{i3}-0.598c_{i4}-0.383c_{i5},
\end{cases}
\end{align}
$$
여기서 $c_{ij}=x_{ij}-\bar{x}_{j}$는 $i$번째 개체의 $j$번째 변수 관찰값 $x_{ij}$에서 $j$번째 변수의 평균값 $\bar{x}_j$을 뺀 것을 의미한다.

------------------

```{r}
# 주성분점수 그래프
par(mfrow = c(3, 1))
# 1. 개체 번호별 그래프 
plot(satis.score[,1:2], xlim = c(-4, 4), ylim = c(-2, 2), main = "개체 번호별 그래프")
abline(v = 0, h = 0, lty = 2)
text(satis.score[,1:2],  labels = satis$ID, pos = 4, col = "red")

# 2. 성별 그래프 
plot(satis.score[,1:2], xlim = c(-4, 4), ylim = c(-2, 2), main = "성별 그래프")
abline(v = 0, h = 0, lty = 2)
text(satis.score[,1:2],  labels = satis$gender, pos = 4, col = "red")

# 3. 연령대별 그래프 
plot(satis.score[,1:2], xlim = c(-4, 4), ylim = c(-2, 2), main = "연령대별 그래프")
abline(v = 0, h = 0, lty = 2)
text(satis.score[,1:2],  labels = satis$age, pos = 4, col = "red")
```

`Result!` 첫 번째 그래프를 살펴보면, 그래프의 오른쪽에 위치하는 "10", "9", "7"번 개체들은 첫 번째 주성분 값이 높으므로 전반적인 만족도가 높은 편이며, 왼쪽에 위치하는 "1", "2", "6"번 개체들은 전반적인 만족도가 낮음을 알 수 있다. 또한, 그래프의 위쪽에 위치하는 "3", "4", "7"번 개체들은 두 번째 주성분 값이 높으므로 가격, 성능, 편리성 등 제품의 내형적 요인에는 만족도가 높으나 디자인, 색상 등 외형적 요인에는 만족도가 낮은 반면, 아래쪽에 위치하는 "5", "8", "9"번 개체들은 외형적 요인에는 만족도가 높으나 내형적 요인에는 만족도가 낮은 편이라고 해석할 수 있다. 성별, 연령대별도 똑같이 해석할 수 있다.

--------------------

```{r}
# 주성분점수 예측
predict(satis.prcomp, newdata = tail(satis[,c("x1", "x2", "x3", "x4", "x5")], 2))
```

`Caution!` 함수 `predict()`를 이용해서 새로운 데이터에 대한 주성분점수를 예측할 수 있다. 여기서는 설명의 편의상 마지막 2개의 자료를 새로운 자료로 취급하여 예측을 수행하였다.  

--------------------

```{r}
# 주성분 분석 행렬도
biplot(satis.prcomp)
```

`Caution!` 주성분 계수(단위 고유벡터)와 주성분점수를 하나의 그래프에 함께 표현한 것을 행렬도(Biplot)라고 한다. 일반적으로 행렬도에서는 개체를 점으로 표현하고, 변수를 벡터로 표현한다. 이러한 행렬도를 통해서 개체들의 군집성, 변수들의 연관구조, 변수별 개체의 상대적 위치 등을 시각적으로 파악할 수 있다.  
`Result!` 화살표를 먼저 살펴보면, 첫 번째 주성분을 기준으로 화살표는 모두 0의 오른쪽에 위치하기에 주성분 계수가 양수임을 알 수 있다. 또한, 두 번째 주성분을 기준으로 변수 $x_1$, $x_2$, $x_3$는 0보다 위쪽에 위치하기 때문에 주성분 계수가 양수이며, 변수 $x_4$, $x_5$는 0보다 아래 쪽에 위치하기에 주성분 계수가 음수이다. 또한, 화살표의 길이는 변수의 분산을 의미하는데 변수 $x_3$가 분산이 가장 작기 때문에 가장 짧다. 화살표끼리 거리가 가깝고 같은 방향일수록 변수들의 상관성이 높아지며, 이는 변수 $x_1$과 $x_3$, 변수 $x_4$와 $x_5$의 상관성이 높다는 것을 의미한다. 각 주성분 축에 가깝게 평행을 이루는 변수가 해당 주성분에 영향을 가장 많이 주는 변수로 해석할 수 있다. 예를 들어, 두 번째 주성분 축과 변수 $x_2$와 $x_4$가 평행에 가까운데 두 변수의 두 번째 주성분 계수는 상대적으로 크다. 그리고, 개체를 살펴보면, 오른쪽 축은 두 번째 주성분 값을 의미하고 위쪽 축은 첫 번째 주성분 값을 의미한다. 즉, "7", "10", "9"번 개체는 첫 번째 주성분점수 값이 양수임을 나타낸다. 게다가, "7", "10"번 개체는 변수 $x_1$, $x_2$, $x_3$에 가까우므로 변수 $x_1$, $x_2$, $x_3$의 값이 높다는 것을 나타내고, "9"번 개체는 변수 $x_4$, $x_5$에 가까우므로 변수 $x_4$, $x_5$의 값이 높다는 것을 나타낸다.

--------------------------

```{r}
# ggbiplot을 이용한 주성분 분석 행렬도
pacman::p_load("devtools")
install_github("vqv/ggbiplot")
library(ggbiplot)

ggbiplot(satis.prcomp,                      # 함수 prcomp에 의한 객체
         obs.scale = 1,                     # 관찰값에 적용할 스케일
         var.scale = 1,                     # 변수에 적용할 스케일
         labels = satis$ID,                 # 점에 대한 라벨
         circle = TRUE) +
  theme_bw()
```

`Caution!` Package `ggbiplot`을 이용해서 주성분 분석 행렬도를 작성할 수 있다.  함수 `ggbiplot()`에 대한 자세한 옵션은 [여기](https://www.rdocumentation.org/packages/ggbiplot/versions/0.55/topics/ggbiplot)를 참고한다.

--------------------------------------

```{r}
# 공분산행렬을 입력으로 하는 주성분 분석
cov.mat <- cov(satis[,c("x1", "x2", "x3", "x4", "x5")])
cov.mat

satis.princomp <- princomp(covmat = cov.mat)
satis.princomp

summary(satis.princomp, 
        loadings = TRUE,      # 주성분 계수(단위 고유벡터) 출력 여부
        cutoff = 0)           # 고유벡터 값 등을 출력할 때 주어진 값보다 작은 값은 출력하지 않도록 지정
```

`Caution!` 함수 `princomp()`는 공분산행렬을 입력 데이터로 하여 주성분 분석을 수행할 수 있다.  
`Result!` 결과는 "satis.prcomp"와 동일하다.

--------------------------

```{r}
# 상관행렬을 이용한 주성분 분석
satis.cor.prcomp <- prcomp(satis[,c("x1", "x2", "x3", "x4", "x5")], center = TRUE, scale = TRUE) 
satis.cor.prcomp
```

`Caution!` 상관행렬에 기초하여 주성분 분석을 수행한다는 것은 표준화 변수를 분석대상으로 주성분 분석을 수행한다는 것이다. 함수 `prcomp()`의 옵션  `center = TRUE`와 `scale = TRUE`를 지정하여 상관행렬에 기초한 주성분 분석을 수행할 수 있다.  
`Result!` 상관행렬을 이용한 주성분 분석 결과를 살펴보면, 위에서 공분산행렬에 기초하여 수행한 주성분 분석 `prcomp(satis[,c("x1", "x2", "x3", "x4", "x5")], center = TRUE, scale = FALSE)` 결과와는 다른 것을 확인할 수 있다.

--------------------------

```{r}
satis.cor.prcomp$sdev        # 주성분의 표준편차
satis.cor.prcomp$sdev^2      # 주성분의 분산 = 주성분에 의해 설명되는 분산의 양 = 상관행렬의 고유값
```

`Caution!` 함수 `eigen(cor(satis[,c("x1", "x2", "x3", "x4", "x5")]))`를 통해 주성분의 분산이 상관행렬의 고유값임을 확인할 수 있으며, 상관행렬의 대각성분 합이 주성분 분산의 합(상관행렬의 고유값의 합)인 변수 개수 "5"와 같음을 알 수 있다.

----------------------------

```{r}
satis.cor.prcomp$rotation    # 주성분 계수
```

`Caution!` 함수 `eigen(cor(satis[,c("x1", "x2", "x3", "x4", "x5")]))`를 통해 주성분 계수가 상관행렬의 단위 고유벡터임을 확인할 수 있다.  
`Result!` 주성분 계수에 의하면, 첫 번째 주성분 $Y_1=0.511Z_{1}+0.407Z_{2}+0.465Z_{3}+0.380Z_{4}+0.462Z_{5}$이며, 두 번째 주성분 $Y_2=0.171Z_{1}+0.500Z_{2}+0.344Z_{3}-0.621Z_{4}-0.466Z_{5}$이 된다. 여기서 확률변수 $Z_{i}$는 $(X_i-\mu_i)/\sigma_{i}$이며, $\mu_i$와 $\sigma_{i}$는 각각 $X_i$의 평균과 표준편차를 의미한다.   
주성분의 의미를 해석해보면, 첫 번째 주성분은 주성분 계수가 서로 비슷하여 전반적인 만족도를 나타낸다고 할 수 있으며, 두 번째 주성분은 변수 $x_1$, $x_2$, $x_3$와 변수 $x_4$, $x_5$의 부호가 반대이므로 가격, 성능, 편리성 제품의 "내형적 요인"과 디자인, 색상 등 제품의 "외형적 요인"의 만족도 차이를 나타낸다고 할 수 있다.

----------------------------

```{r}
# 요약
summary(satis.cor.prcomp)    
```

`Result!` 첫 2개의 고유값은 3.108, 1.399로서 각각 전체 변동의 62.2%(=3.108/5)와 28.0%(=1.399/5)를 차지한다. 따라서, 첫 2개의 주성분에 의해 데이터 변동의 약 90.1%가 설명될 수 있음을 알 수 있다. 이는 2개의 주성분으로 데이터 변동을 충분히 설명할 수 있음을 가리키며, 만약 첫 2개의 주성분을 취할 경우 이는 원래 다섯 개의 관찰변수들이 가지고 있는 정보를 2차원으로 축약한다는 의미를 가진다. 

------------------

```{r}
# 스크리 그래프
par(mfrow = c(1, 2))
screeplot(satis.cor.prcomp, type = "b", main = "")  # 막대그래프
screeplot(satis.cor.prcomp, type = "l", main = "")  # 선 그래프
```

`Result!` 스트리 그래프를 보면 첫 번째 주성분과 두 번째 주성분의 분산(고유값)이 크며 가파른 경사를 형성한다. 그러므로, 주성분 개수는 2개로 선정할 수 있다.

---------------------------------

```{r}
# 주성분점수
satis.cor.score <- satis.cor.prcomp$x           
satis.cor.score
```

`Result!` 첫 번째 주성분과 두 번째 주성분에 의해 얻어지는 $i$번째 개체의 주성분점수는 다음과 같다.
$$
\begin{align}
\begin{cases}
y_{i1}=0.511z_{i1}+0.407z_{i2}+0.465z_{i3}+0.380z_{i4}+0.462z_{i5},\\
y_{i2}=0.171z_{i1}+0.500z_{i2}+0.344z_{i3}-0.621z_{i4}-0.466z_{i5},
\end{cases}
\end{align}
$$
여기서 $z_{ij}=(x_{ij}-\bar{x}_{j})/s_{j}$는 $i$번째 개체의 $j$번째 변수 관찰값 $x_{ij}$에서 $j$번째 변수의 평균값 $\bar{x}_j$을 뺀 후 $j$번째 변수의 표준편차 값 $s_j$로 나눈 것을 의미한다.


----------------------------------------

```{r}
# 상관행렬을 입력으로 하는 주성분 분석
cor.mat <- cor(satis[,c("x1", "x2", "x3", "x4", "x5")])
cor.mat

satis.cor.princomp <- princomp(covmat = cor.mat)
satis.cor.princomp

summary(satis.cor.princomp, 
        loadings = TRUE,      # 주성분 계수(단위 고유벡터) 출력 여부
        cutoff = 0)           # 고유벡터 값 등을 출력할 때 주어진 값보다 작은 값은 출력하지 않도록 지정
```

`Caution!` 함수 `princomp()`는 상관행렬을 입력 데이터로 하여 주성분 분석을 수행할 수 있다.  
`Result!` 결과는 "satis.cor.prcomp"와 동일하다.

----------------------------------------

## **7-2. 학생성적 데이터**

- [자유아카데미](http://www.freeaca.com/new/library/BoardFileList.aspx?page=1&sword=%eb%8b%a4%eb%b3%80%eb%9f%89&stype=title&area=2)에서 출판한 책 **R을 활용한 다변량 자료분석 방법론**의 데이터 파일 중 "student.csv"를 활용한다.
- 이 데이터는 10명의 학생에 대한 5과목 점수 데이터로 총 6개의 변수로 이루어져 있다.
    1. ID : 학생 ID
    2. $x_1$ : 국어 점수
    3. $x_2$ : 영어 점수
    4. $x_3$ : 제2외국어 점수
    5. $x_4$ : 수학 점수
    6. $x_5$ : 과학 점수

    
```{r}
# 데이터 불러오기
student <- read.csv("C:/Users/User/Desktop/student.csv")
student
```

----------------------------

```{r}
# 기술통계량 출력
pacman::p_load("psych")
describe(student[,c("x1", "x2", "x3", "x4", "x5")])
```

`Result!` 변수 $x_1$의 표준편차는 다른 변수들에 비해 상대적으로 크지만, 변수 $x_3$는 다른 변수들에 비해 상대적으로 표준편차가 작다.

-----------------

```{r}
# 상관행렬 출력
cor(student[,c("x1", "x2", "x3", "x4", "x5")])
```

`Result!` 상대적으로 표준편차가 큰 변수 $x_1$은 상대적으로 다른 변수들과 0.5 이상의 높은 상관성을 보인다. 반면, 상대적으로 표준편차가 작은 변수 $x_3$는 변수 $x_4$, $x_5$와 상관계수가 0.3 이하로 낮은 상관성을 보인다. 그리고, 변수 $x_4$와 $x_5$ 사이에는 매우 큰 양의 상관성을 가지고 있다($r=0.97$). 

-----------------

```{r}
# 공분산행렬을 이용한 주성분 분석
student.prcomp <- prcomp(student[,c("x1", "x2", "x3", "x4", "x5")]) 
student.prcomp
```

`Result!` 함수 `prcomp()`로부터 얻어진 리스트 객체에는 각 결과들이 별도로 저장되어 있다. 첫 번째 결과 "Standard deviations"에는 주성분의 표준편차(즉, 공분산행렬의 고유값의 제곱근)이 저장되어 있다. 두 번째 결과 "Rotation"에는 주성분 계수(즉, 공분산행렬의 단위 고유벡터)가 저장되어 있다.

----------------

```{r}
student.prcomp$sdev        # 주성분의 표준편차
student.prcomp$sdev^2      # 주성분의 분산 = 주성분에 의해 설명되는 분산의 양 = 공분산행렬의 고유값
```

`Caution!` 함수 `eigen(cov(student[,c("x1", "x2", "x3", "x4", "x5")]))`를 통해 주성분의 분산이 공분산행렬의 고유값임을 확인할 수 있으며, 공분산행렬의 대각성분 합이 주성분 분산의 합(공분산행렬의 고유값의 합)과 같음을 알 수 있다.

----------------

```{r}
# 주성분 계수
student.prcomp$rotation   
```

`Caution!` 함수 `eigen(cov(student[,c("x1", "x2", "x3", "x4", "x5")]))`를 통해 주성분 계수가 공분산행렬의 단위 고유벡터임을 확인할 수 있다.  
`Result!` 주성분 계수에 의하면, 첫 번째 주성분 $Y_1=0.616C_{1}+0.315C_{2}+0.153C_{3}+0.521C_{4}+0.477C_{5}$이며, 두 번째 주성분 $Y_2=0.418C_1+0.540C_2+0.287C_3-0.544C_4-0.395C_{5}$이 된다. 여기서 확률변수 $C_{i}$는 $X_i-\mu_i$이며, $\mu_i$는 $X_i$의 평균을 의미한다. 즉, 함수 `prcomp()`는 기본적으로 중심화(평균이 0)된 데이터행렬을 대상으로 한다. 즉, 위의 분석 결과는 `prcomp(dat.x, center = TRUE, scale = FALSE)`와 동일하다.  
주성분의 의미를 해석해보면, 첫 번째 주성분은 주성분 계수가 서로 비슷하여 전반적인 평점을 나타낸다고 할 수 있으며, 두 번째 주성분은 변수 $x_1$, $x_2$, $x_3$와 변수 $x_4$, $x_5$의 부호가 반대이므로 국어, 영어, 제2외국어의 "인문계열"과 수학, 과학의 "이과계열"의 점수 차이를 나타낸다고 할 수 있다.

----------------

```{r}
# 요약
summary(student.prcomp)    
```

`Result!` 첫 2개의 고유값은 2139.972, 772.956로서 각각 전체 변동의 69.7%(=2139.972/3071.411)와 25.2%(=772.956/3071.411)를 차지한다. 따라서, 첫 2개의 주성분에 의해 데이터 변동의 약 94.8%가 설명될 수 있음을 알 수 있다. 이는 2개의 주성분으로 데이터 변동을 충분히 설명할 수 있음을 가리키며, 만약 첫 2개의 주성분을 취할 경우 이는 원래 다섯 개의 관찰변수들이 가지고 있는 정보를 2차원으로 축약한다는 의미를 가진다. 

------------------

```{r}
# 스크리 그래프
par(mfrow = c(1, 2))
screeplot(student.prcomp, type = "b", main = "")  # 막대그래프
screeplot(student.prcomp, type = "l", main = "")  # 선 그래프
```

`Result!` 스트리 그래프를 보면 첫 번째 주성분과 두 번째 주성분의 분산(고유값)이 크며 가파른 경사를 형성한다. 그러므로, 주성분 개수는 2개로 선정할 수 있다.

------------------

```{r}
# 주성분점수
student.score <- student.prcomp$x           
student.score
```

`Result!` 첫 번째 주성분과 두 번째 주성분에 의해 얻어지는 $i$번째 개체의 주성분점수는 다음과 같다.
$$
\begin{align}
\begin{cases}
y_{i1}=0.616c_{i1}+0.315c_{i2}+0.153c_{i3}+0.521c_{i3}+0.477c_{i5},\\
y_{i2}=0.418c_{i1}+0.540c_{i2}+0.287c_{i3}-0.544c_{i4}-0.395c_{i5},
\end{cases}
\end{align}
$$
여기서 $c_{ij}=x_{ij}-\bar{x}_{j}$는 $i$번째 개체의 $j$번째 변수 관찰값 $x_{ij}$에서 $j$번째 변수의 평균값 $\bar{x}_j$을 뺀 것을 의미한다.

------------------

```{r}
# 주성분점수 그래프
plot(student.score[,1:2], xlim = c(-80, 80), ylim = c(-50, 50), main = "개체 번호별 그래프")
abline(v = 0, h = 0, lty = 2)
text(student.score[,1:2],  labels = student$ID, pos = 4, col = "red")
```

`Result!` 그래프를 살펴보면, 그래프의 오른쪽에 위치하는 "10", "9", "7"번 개체들은 첫 번째 주성분 값이 높으므로 전반적인 평점이 높은 편이며, 왼쪽에 위치하는 "1", "2", "6"번 개체들은 전반적인 평점이 낮은 편임을 알 수 있다. 또한, 그래프의 위쪽에 위치하는 "3", "4", "7"번 개체들은 두 번째 주성분 값이 높으므로 국어, 영어, 제2외국어 등 인문계열 과목에는 점수가 높으나 수학, 과학 등 이과계열 과목에는 점수가 낮은 반면, 아래쪽에 위치하는 "5", "8"번 개체들은 이과계열 과목에는 점수가 높으나 인문계열 과목에는 만족도가 낮은 편이라고 해석할 수 있다.

--------------------

```{r}
# 주성분점수 예측
predict(student.prcomp, newdata = tail(student[,c("x1", "x2", "x3", "x4", "x5")], 2))
```

`Caution!` 함수 `predict()`를 이용해서 새로운 데이터에 대한 주성분점수를 예측할 수 있다. 여기서는 설명의 편의상 마지막 2개의 자료를 새로운 자료로 취급하여 예측을 수행하였다.  

--------------------

```{r}
# 주성분 분석 행렬도
biplot(student.prcomp)
```


`Result!` 화살표를 먼저 살펴보면, 첫 번째 주성분을 기준으로 화살표는 모두 0의 오른쪽에 위치하기에 주성분 계수가 양수임을 알 수 있다. 또한, 두 번째 주성분을 기준으로 변수 $x_1$, $x_2$, $x_3$는 0보다 위쪽에 위치하기 때문에 주성분 계수가 양수이며, 변수 $x_4$, $x_5$는 0보다 아래 쪽에 위치하기에 주성분 계수가 음수이다. 또한, 화살표의 길이는 변수의 분산을 의미하는데 변수 $x_3$가 분산이 가장 작기 때문에 가장 짧다. 화살표끼리 거리가 가깝고 같은 방향일수록 변수들의 상관성이 높아지며, 이는 변수 $x_2$와 $x_3$, 변수 $x_4$와 $x_5$의 상관성이 높다는 것을 의미한다. 그리고, 개체를 살펴보면, 오른쪽 축은 두 번째 주성분 값을 의미하고 위쪽 축은 첫 번째 주성분 값을 의미한다. 즉, "7", "10", "9"번 개체는 첫 번째 주성분점수 값이 양수임을 나타낸다. 게다가, "7", "3", "4"번 개체는 변수 $x_2$와 $x_3$에 가까우므로 두 변수의 값이 높다는 것을 나타내고, 변수 $x_1$에 가까운 "10"번 개체는 국어 점수가 높다는 것을 나타낸다. 

--------------------------

```{r}
# ggbiplot을 이용한 주성분 분석 행렬도
pacman::p_load("devtools")
install_github("vqv/ggbiplot")
library(ggbiplot)

ggbiplot(student.prcomp,                    # 함수 prcomp에 의한 객체
         obs.scale = 1,                     # 관찰값에 적용할 스케일
         var.scale = 1,                     # 변수에 적용할 스케일
         labels = student$ID,               # 점에 대한 라벨
         circle = TRUE) +
  theme_bw()
```

--------------------------------------

```{r}
# 공분산행렬을 입력으로 하는 주성분 분석
cov.mat <- cov(student[,c("x1", "x2", "x3", "x4", "x5")])
cov.mat

student.princomp <- princomp(covmat = cov.mat)
student.princomp

summary(student.princomp, 
        loadings = TRUE,      # 주성분 계수(단위 고유벡터) 출력 여부
        cutoff = 0)           # 고유벡터 값 등을 출력할 때 주어진 값보다 작은 값은 출력하지 않도록 지정
```

`Result!` 결과는 "student.prcomp"와 동일하다.

--------------------------

```{r}
# 상관행렬을 이용한 주성분 분석
student.cor.prcomp <- prcomp(student[,c("x1", "x2", "x3", "x4", "x5")], center = TRUE, scale = TRUE) 
student.cor.prcomp
```

`Result!` 상관행렬을 이용한 주성분 분석 결과를 살펴보면, 위에서 공분산행렬에 기초하여 수행한 주성분 분석 `prcomp(satis[,c("x1", "x2", "x3", "x4", "x5")], center = TRUE, scale = FALSE)` 결과와는 다른 것을 확인할 수 있다.

--------------------------

```{r}
student.cor.prcomp$sdev        # 주성분의 표준편차
student.cor.prcomp$sdev^2      # 주성분의 분산 = 주성분에 의해 설명되는 분산의 양 = 상관행렬의 고유값
```

`Caution!` 함수 `eigen(cor(student[,c("x1", "x2", "x3", "x4", "x5")]))`를 통해 주성분의 분산이 상관행렬의 고유값임을 확인할 수 있으며, 상관행렬의 대각성분 합이 주성분 분산의 합(상관행렬의 고유값의 합)인 변수 개수 "5"와 같음을 알 수 있다.

----------------------------

```{r}
student.cor.prcomp$rotation    # 주성분 계수
```

`Result!` 주성분 계수에 의하면, 첫 번째 주성분 $Y_1=0.518Z_{1}+0.452Z_{2}+0.421Z_{3}+0.398Z_{4}+0.439Z_{5}$이며, 두 번째 주성분 $Y_2=0.094Z_{1}+0.438Z_{2}+0.474Z_{3}-0.574Z_{4}-0.495Z_{5}$이 된다. 여기서 확률변수 $Z_{i}$는 $(X_i-\mu_i)/\sigma_{i}$이며, $\mu_i$와 $\sigma_{i}$는 각각 $X_i$의 평균과 표준편차를 의미한다.   
주성분의 의미를 해석해보면, 첫 번째 주성분은 주성분 계수가 서로 비슷하여 전반적인 평점을 나타낸다고 할 수 있으며, 두 번째 주성분은 변수 $x_1$, $x_2$, $x_3$와 변수 $x_4$, $x_5$의 부호가 반대이므로 국어, 영어, 제2외국어의 "인문계열"과 수학, 과학의 "이과계열"의 점수 차이를 나타낸다고 할 수 있다.

----------------------------

```{r}
# 요약
summary(student.cor.prcomp)    
```

`Result!` 첫 2개의 고유값은 3.187, 1.472로서 각각 전체 변동의 63.7%(=3.187/5)와 29.4%(=1.472/5)를 차지한다. 따라서, 첫 2개의 주성분에 의해 데이터 변동의 약 93.2%가 설명될 수 있음을 알 수 있다. 이는 2개의 주성분으로 데이터 변동을 충분히 설명할 수 있음을 가리키며, 만약 첫 2개의 주성분을 취할 경우 이는 원래 다섯 개의 관찰변수들이 가지고 있는 정보를 2차원으로 축약한다는 의미를 가진다. 

------------------

```{r}
# 스크리 그래프
par(mfrow = c(1, 2))
screeplot(student.cor.prcomp, type = "b", main = "")  # 막대그래프
screeplot(student.cor.prcomp, type = "l", main = "")  # 선 그래프
```

`Result!` 스트리 그래프를 보면 첫 번째 주성분과 두 번째 주성분의 분산(고유값)이 크며 가파른 경사를 형성한다. 그러므로, 주성분 개수는 2개로 선정할 수 있다.

---------------------------------

```{r}
# 주성분점수
student.cor.score <- student.cor.prcomp$x           
student.cor.score
```

`Result!` 첫 번째 주성분과 두 번째 주성분에 의해 얻어지는 $i$번째 개체의 주성분점수는 다음과 같다.
$$
\begin{align}
\begin{cases}
y_{i1}=0.518z_{i1}+0.452z_{i2}+0.421z_{i3}+0.398z_{i4}+0.439z_{i5},\\
y_{i2}=0.094z_{i1}+0.438z_{i2}+0.474z_{i3}-0.574z_{i4}-0.495z_{i5},
\end{cases}
\end{align}
$$
여기서 $z_{ij}=(x_{ij}-\bar{x}_{j})/s_{j}$는 $i$번째 개체의 $j$번째 변수 관찰값 $x_{ij}$에서 $j$번째 변수의 평균값 $\bar{x}_j$을 뺀 후 $j$번째 변수의 표준편차 값 $s_j$로 나눈 것을 의미한다.


----------------------------------------

```{r}
# 상관행렬을 입력으로 하는 주성분 분석
cor.mat <- cor(student[,c("x1", "x2", "x3", "x4", "x5")])
cor.mat

student.cor.princomp <- princomp(covmat = cor.mat)
student.cor.princomp

summary(student.cor.princomp, 
        loadings = TRUE,      # 주성분 계수(단위 고유벡터) 출력 여부
        cutoff = 0)           # 고유벡터 값 등을 출력할 때 주어진 값보다 작은 값은 출력하지 않도록 지정
```

`Result!` 결과는 "student.cor.prcomp"와 동일하다.
